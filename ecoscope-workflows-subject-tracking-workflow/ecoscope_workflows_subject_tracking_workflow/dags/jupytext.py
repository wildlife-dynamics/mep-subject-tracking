# AUTOGENERATED BY ECOSCOPE-WORKFLOWS; see fingerprint in README.md for details


# ruff: noqa: E402

# %% [markdown]
# # Subject Tracking
# TODO: top level description

# %% [markdown]
# ## Imports

import os

from ecoscope_workflows_core.tasks.analysis import (
    dataframe_column_nunique as dataframe_column_nunique,
)
from ecoscope_workflows_core.tasks.analysis import (
    dataframe_column_sum as dataframe_column_sum,
)
from ecoscope_workflows_core.tasks.config import set_string_var as set_string_var
from ecoscope_workflows_core.tasks.config import (
    set_workflow_details as set_workflow_details,
)
from ecoscope_workflows_core.tasks.filter import set_time_range as set_time_range
from ecoscope_workflows_core.tasks.groupby import set_groupers as set_groupers
from ecoscope_workflows_core.tasks.groupby import split_groups as split_groups
from ecoscope_workflows_core.tasks.io import persist_text as persist_text
from ecoscope_workflows_core.tasks.io import set_er_connection as set_er_connection
from ecoscope_workflows_core.tasks.io import set_gee_connection as set_gee_connection
from ecoscope_workflows_core.tasks.results import (
    create_map_widget_single_view as create_map_widget_single_view,
)
from ecoscope_workflows_core.tasks.results import (
    create_single_value_widget_single_view as create_single_value_widget_single_view,
)
from ecoscope_workflows_core.tasks.results import gather_dashboard as gather_dashboard
from ecoscope_workflows_core.tasks.results import (
    merge_widget_views as merge_widget_views,
)
from ecoscope_workflows_core.tasks.skip import (
    any_dependency_skipped as any_dependency_skipped,
)
from ecoscope_workflows_core.tasks.skip import any_is_empty_df as any_is_empty_df
from ecoscope_workflows_core.tasks.skip import never as never
from ecoscope_workflows_core.tasks.transformation import (
    add_temporal_index as add_temporal_index,
)
from ecoscope_workflows_core.tasks.transformation import map_columns as map_columns
from ecoscope_workflows_core.tasks.transformation import sort_values as sort_values
from ecoscope_workflows_ext_custom.tasks.io import html_to_png as html_to_png
from ecoscope_workflows_ext_custom.tasks.io import load_df as load_df
from ecoscope_workflows_ext_custom.tasks.results import (
    create_geojson_layer as create_geojson_layer,
)
from ecoscope_workflows_ext_custom.tasks.results import (
    create_path_layer as create_path_layer,
)
from ecoscope_workflows_ext_custom.tasks.results import draw_map as draw_map
from ecoscope_workflows_ext_custom.tasks.results import (
    set_base_maps_pydeck as set_base_maps_pydeck,
)
from ecoscope_workflows_ext_custom.tasks.transformation import (
    filter_row_values as filter_row_values,
)
from ecoscope_workflows_ext_custom.tasks.transformation import (
    to_quantity as to_quantity,
)
from ecoscope_workflows_ext_ecoscope.tasks.analysis import (
    calculate_elliptical_time_density as calculate_elliptical_time_density,
)
from ecoscope_workflows_ext_ecoscope.tasks.io import (
    determine_season_windows as determine_season_windows,
)
from ecoscope_workflows_ext_ecoscope.tasks.io import get_events as get_events
from ecoscope_workflows_ext_ecoscope.tasks.io import (
    get_subjectgroup_observations as get_subjectgroup_observations,
)
from ecoscope_workflows_ext_ecoscope.tasks.io import persist_df as persist_df
from ecoscope_workflows_ext_ecoscope.tasks.preprocessing import (
    process_relocations as process_relocations,
)
from ecoscope_workflows_ext_ecoscope.tasks.preprocessing import (
    relocations_to_trajectory as relocations_to_trajectory,
)
from ecoscope_workflows_ext_ecoscope.tasks.transformation import (
    apply_classification as apply_classification,
)
from ecoscope_workflows_ext_ecoscope.tasks.transformation import (
    apply_color_map as apply_color_map,
)
from ecoscope_workflows_ext_ecoscope.tasks.transformation import (
    normalize_json_column as normalize_json_column,
)
from ecoscope_workflows_ext_mep.tasks import (
    build_template_region_lookup as build_template_region_lookup,
)
from ecoscope_workflows_ext_mep.tasks import compute_maturity as compute_maturity
from ecoscope_workflows_ext_mep.tasks import (
    compute_subject_occupancy as compute_subject_occupancy,
)
from ecoscope_workflows_ext_mep.tasks import (
    compute_subject_stats as compute_subject_stats,
)
from ecoscope_workflows_ext_mep.tasks import (
    compute_template_regions as compute_template_regions,
)
from ecoscope_workflows_ext_mep.tasks import (
    create__mep_context_page as create__mep_context_page,
)
from ecoscope_workflows_ext_mep.tasks import (
    create_mep_ctx_cover as create_mep_ctx_cover,
)
from ecoscope_workflows_ext_mep.tasks import (
    create_mep_grouper_page as create_mep_grouper_page,
)
from ecoscope_workflows_ext_mep.tasks import (
    create_mep_subject_context as create_mep_subject_context,
)
from ecoscope_workflows_ext_mep.tasks import (
    custom_view_state_deck_gdf as custom_view_state_deck_gdf,
)
from ecoscope_workflows_ext_mep.tasks import (
    draw_season_collared_plot as draw_season_collared_plot,
)
from ecoscope_workflows_ext_mep.tasks import (
    draw_season_mcp_plot as draw_season_mcp_plot,
)
from ecoscope_workflows_ext_mep.tasks import (
    draw_season_nsd_plot as draw_season_nsd_plot,
)
from ecoscope_workflows_ext_mep.tasks import (
    draw_season_speed_plot as draw_season_speed_plot,
)
from ecoscope_workflows_ext_mep.tasks import get_subject_df as get_subject_df
from ecoscope_workflows_ext_mep.tasks import (
    persist_subject_photo as persist_subject_photo,
)
from ecoscope_workflows_ext_mep.tasks import (
    process_subject_information as process_subject_information,
)
from ecoscope_workflows_ext_mep.tasks import (
    zoom_map_and_screenshot as zoom_map_and_screenshot,
)
from ecoscope_workflows_ext_mnc.tasks import transform_columns as transform_columns
from ecoscope_workflows_ext_ste.tasks import (
    annotate_gdf_dict_with_geom_type as annotate_gdf_dict_with_geom_type,
)
from ecoscope_workflows_ext_ste.tasks import (
    calculate_seasonal_home_range as calculate_seasonal_home_range,
)
from ecoscope_workflows_ext_ste.tasks import (
    combine_deckgl_map_layers as combine_deckgl_map_layers,
)
from ecoscope_workflows_ext_ste.tasks import convert_to_str as convert_to_str
from ecoscope_workflows_ext_ste.tasks import (
    create_custom_text_layer as create_custom_text_layer,
)
from ecoscope_workflows_ext_ste.tasks import (
    create_deckgl_layers_from_gdf_dict as create_deckgl_layers_from_gdf_dict,
)
from ecoscope_workflows_ext_ste.tasks import (
    create_seasonal_labels as create_seasonal_labels,
)
from ecoscope_workflows_ext_ste.tasks import (
    custom_trajectory_segment_filter as custom_trajectory_segment_filter,
)
from ecoscope_workflows_ext_ste.tasks import envelope_gdf as envelope_gdf
from ecoscope_workflows_ext_ste.tasks import (
    fetch_and_persist_file as fetch_and_persist_file,
)
from ecoscope_workflows_ext_ste.tasks import filter_df_cols as filter_df_cols
from ecoscope_workflows_ext_ste.tasks import generate_mcp_gdf as generate_mcp_gdf
from ecoscope_workflows_ext_ste.tasks import get_file_path as get_file_path
from ecoscope_workflows_ext_ste.tasks import merge_mapbook_files as merge_mapbook_files
from ecoscope_workflows_ext_ste.tasks import split_gdf_by_column as split_gdf_by_column
from ecoscope_workflows_ext_ste.tasks import view_state_deck_gdf as view_state_deck_gdf
from ecoscope_workflows_ext_ste.tasks import zip_groupbykey as zip_groupbykey

# %% [markdown]
# ## Set workflow details

# %%
# parameters

workflow_details_params = dict(
    name=...,
    description=...,
    image_url=...,
)

# %%
# call the task


workflow_details = (
    set_workflow_details.set_task_instance_id("workflow_details")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(**workflow_details_params)
    .call()
)


# %% [markdown]
# ## Define analysis time range

# %%
# parameters

time_range_params = dict(
    since=...,
    until=...,
    timezone=...,
    time_format=...,
)

# %%
# call the task


time_range = (
    set_time_range.set_task_instance_id("time_range")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(**time_range_params)
    .call()
)


# %% [markdown]
# ## Configure grouping strategy

# %%
# parameters

groupers_params = dict()

# %%
# call the task


groupers = (
    set_groupers.set_task_instance_id("groupers")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(groupers=[{"index_name": "subject_name"}], **groupers_params)
    .call()
)


# %% [markdown]
# ## Configure base map layers

# %%
# parameters

configure_base_maps_params = dict(
    base_maps=...,
)

# %%
# call the task


configure_base_maps = (
    set_base_maps_pydeck.set_task_instance_id("configure_base_maps")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(**configure_base_maps_params)
    .call()
)


# %% [markdown]
# ## Connect to earth ranger

# %%
# parameters

er_client_name_params = dict(
    data_source=...,
)

# %%
# call the task


er_client_name = (
    set_er_connection.set_task_instance_id("er_client_name")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(**er_client_name_params)
    .call()
)


# %% [markdown]
# ## Connect to earth engine

# %%
# parameters

gee_project_name_params = dict(
    data_source=...,
)

# %%
# call the task


gee_project_name = (
    set_gee_connection.set_task_instance_id("gee_project_name")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(**gee_project_name_params)
    .call()
)


# %% [markdown]
# ##

# %%
# parameters

subject_group_var_params = dict(
    var=...,
)

# %%
# call the task


subject_group_var = (
    set_string_var.set_task_instance_id("subject_group_var")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(**subject_group_var_params)
    .call()
)


# %% [markdown]
# ## Load landDx database

# %%
# parameters

retrieve_ldx_db_params = dict(
    input_method=...,
)

# %%
# call the task


retrieve_ldx_db = (
    get_file_path.set_task_instance_id("retrieve_ldx_db")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        output_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"], **retrieve_ldx_db_params
    )
    .call()
)


# %% [markdown]
# ## Load ldx gpkg

# %%
# parameters

load_ldx_params = dict()

# %%
# call the task


load_ldx = (
    load_df.set_task_instance_id("load_ldx")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        file_path=retrieve_ldx_db,
        layer="landDx_polygons",
        deserialize_json=False,
        **load_ldx_params,
    )
    .call()
)


# %% [markdown]
# ## Filter loaded landDx by AOI

# %%
# parameters

filter_ldx_aoi_params = dict()

# %%
# call the task


filter_ldx_aoi = (
    filter_row_values.set_task_instance_id("filter_ldx_aoi")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        df=load_ldx,
        column="type",
        values=["Community Conservancy", "National Reserve", "National Park"],
        **filter_ldx_aoi_params,
    )
    .call()
)


# %% [markdown]
# ## Exclude unnecessary columns from ldx gdf

# %%
# parameters

filter_ldx_cols_params = dict()

# %%
# call the task


filter_ldx_cols = (
    filter_df_cols.set_task_instance_id("filter_ldx_cols")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        df=filter_ldx_aoi,
        columns=["type", "name", "geometry"],
        **filter_ldx_cols_params,
    )
    .call()
)


# %% [markdown]
# ## Create text layer

# %%
# parameters

create_ldx_text_layer_params = dict()

# %%
# call the task


create_ldx_text_layer = (
    create_custom_text_layer.set_task_instance_id("create_ldx_text_layer")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        geodataframe=filter_ldx_cols,
        layer_style={
            "get_text": "name",
            "get_color": [0, 0, 0, 255],
            "get_size": 1000,
            "size_units": "meters",
            "size_min_pixels": 40,
            "size_max_pixels": 75,
            "size_scale": 1.25,
            "font_family": "Arial",
            "font_weight": "normal",
            "get_text_anchor": "middle",
            "get_alignment_baseline": "center",
            "billboard": True,
            "background_padding": [4, 8],
            "pickable": True,
            "auto_highlight": False,
        },
        use_centroid=True,
        legend=None,
        **create_ldx_text_layer_params,
    )
    .call()
)


# %% [markdown]
# ## Split ldx gdf by type column

# %%
# parameters

split_ldx_by_type_params = dict()

# %%
# call the task


split_ldx_by_type = (
    split_gdf_by_column.set_task_instance_id("split_ldx_by_type")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(gdf=filter_ldx_cols, column="type", **split_ldx_by_type_params)
    .call()
)


# %% [markdown]
# ## Annotate gdf dict with geom type

# %%
# parameters

annotate_gdf_dict_params = dict()

# %%
# call the task


annotate_gdf_dict = (
    annotate_gdf_dict_with_geom_type.set_task_instance_id("annotate_gdf_dict")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(gdf_dict=split_ldx_by_type, **annotate_gdf_dict_params)
    .call()
)


# %% [markdown]
# ## Create deckgl layers from split ldx gdf dict

# %%
# parameters

create_ldx_styled_layers_params = dict()

# %%
# call the task


create_ldx_styled_layers = (
    create_deckgl_layers_from_gdf_dict.set_task_instance_id("create_ldx_styled_layers")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        gdf_dict=annotate_gdf_dict,
        styles={
            "Community Conservancy": {
                "get_fill_color": [166, 182, 151],
                "get_line_color": [166, 182, 151],
                "opacity": 0.15,
                "stroked": True,
                "get_line_width": 2.0,
            },
            "National Reserve": {
                "get_fill_color": [136, 167, 142],
                "get_line_color": [136, 167, 142],
                "opacity": 0.15,
                "stroked": True,
                "get_line_width": 2.0,
            },
            "National Park": {
                "get_fill_color": [17, 86, 49],
                "get_line_color": [17, 86, 49],
                "opacity": 0.15,
                "stroked": True,
                "get_line_width": 2.0,
            },
        },
        legends={
            "title": "Land Use",
            "values": [
                {"label": "Community Conservancy", "color": "#a6b697"},
                {"label": "National Reserve", "color": "#88a78e"},
                {"label": "National Park", "color": "#115631"},
            ],
        },
        **create_ldx_styled_layers_params,
    )
    .call()
)


# %% [markdown]
# ## Retrieve subjects df from ER

# %%
# parameters

retrieve_subjects_df_params = dict()

# %%
# call the task


retrieve_subjects_df = (
    get_subject_df.set_task_instance_id("retrieve_subjects_df")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        client=er_client_name,
        include_inactive=True,
        bbox=None,
        subject_group_id=None,
        subject_group_name=subject_group_var,
        name=None,
        updated_since=None,
        updated_until=None,
        tracks=None,
        ids=None,
        max_ids_per_request=50,
        raise_on_empty=False,
        **retrieve_subjects_df_params,
    )
    .call()
)


# %% [markdown]
# ## Normalize subject information

# %%
# parameters

normalize_subject_info_params = dict()

# %%
# call the task


normalize_subject_info = (
    normalize_json_column.set_task_instance_id("normalize_subject_info")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        column="additional",
        df=retrieve_subjects_df,
        skip_if_not_exists=True,
        sort_columns=True,
        **normalize_subject_info_params,
    )
    .call()
)


# %% [markdown]
# ## Rename additional subject columns

# %%
# parameters

rename_subject_cols_params = dict()

# %%
# call the task


rename_subject_cols = (
    transform_columns.set_task_instance_id("rename_subject_cols")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        drop_columns=[
            "url",
            "image_url",
            "common_name",
            "content_type",
            "additional__external_id",
            "additional__tm_animal_id",
            "additional__external_name",
        ],
        retain_columns=[],
        rename_columns={
            "id": "groupby_col",
            "name": "subject_name",
            "hex": "hex_color",
            "additional__rgb": "rgb",
            "additional__Bio": "subject_bio",
            "additional__sex": "subject_sex",
            "additional__DOB": "date_of_birth",
            "additional__notes": "notes",
            "additional__status": "status",
            "additional__region": "region",
            "additional__country": "country",
            "additional__id_photo": "photo",
            "additional__distribution": "distribution",
        },
        skip_missing_rename=True,
        required_columns=["id", "name"],
        df=normalize_subject_info,
        **rename_subject_cols_params,
    )
    .call()
)


# %% [markdown]
# ## Choose subject group to analyze

# %%
# parameters

subject_observations_params = dict()

# %%
# call the task


subject_observations = (
    get_subjectgroup_observations.set_task_instance_id("subject_observations")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        filter="clean",
        client=er_client_name,
        time_range=time_range,
        subject_group_name=subject_group_var,
        raise_on_empty=False,
        include_details=False,
        include_subjectsource_details=False,
        **subject_observations_params,
    )
    .call()
)


# %% [markdown]
# ## Transform observations to relocations

# %%
# parameters

subject_reloc_params = dict()

# %%
# call the task


subject_reloc = (
    process_relocations.set_task_instance_id("subject_reloc")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        observations=subject_observations,
        relocs_columns=[
            "groupby_col",
            "fixtime",
            "junk_status",
            "geometry",
            "extra__subject__name",
            "extra__subject__hex",
            "extra__subject__sex",
            "extra__created_at",
            "extra__subject__subject_subtype",
        ],
        filter_point_coords=[
            {"x": 180.0, "y": 90.0},
            {"x": 0.0, "y": 0.0},
            {"x": 1.0, "y": 1.0},
        ],
        **subject_reloc_params,
    )
    .call()
)


# %% [markdown]
# ## Rename reloc columns

# %%
# parameters

rename_reloc_cols_params = dict()

# %%
# call the task


rename_reloc_cols = (
    map_columns.set_task_instance_id("rename_reloc_cols")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        raise_if_not_found=True,
        drop_columns=[],
        retain_columns=[],
        rename_columns={
            "name": "subject_name",
            "hex": "hex_color",
            "sex": "subject_sex",
            "subject_subtype": "subject_subtype",
            "created_at": "created_at",
        },
        df=subject_reloc,
        **rename_reloc_cols_params,
    )
    .call()
)


# %% [markdown]
# ## Compute subject maturity

# %%
# parameters

compute_subject_maturity_params = dict()

# %%
# call the task


compute_subject_maturity = (
    compute_maturity.set_task_instance_id("compute_subject_maturity")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        subject_df=rename_subject_cols,
        relocations_gdf=rename_reloc_cols,
        months_duration=6,
        time_column="fixtime",
        **compute_subject_maturity_params,
    )
    .call()
)


# %% [markdown]
# ## split subjects df by group

# %%
# parameters

split_subject_by_group_params = dict()

# %%
# call the task


split_subject_by_group = (
    split_groups.set_task_instance_id("split_subject_by_group")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        df=compute_subject_maturity, groupers=groupers, **split_subject_by_group_params
    )
    .call()
)


# %% [markdown]
# ## Download subject profile photo

# %%
# parameters

download_profile_pic_params = dict()

# %%
# call the task


download_profile_pic = (
    persist_subject_photo.set_task_instance_id("download_profile_pic")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        image_type=".png",
        column="photo",
        overwrite_existing=True,
        output_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        **download_profile_pic_params,
    )
    .mapvalues(argnames=["subject_df"], argvalues=split_subject_by_group)
)


# %% [markdown]
# ## Download subject information

# %%
# parameters

download_subject_info_params = dict()

# %%
# call the task


download_subject_info = (
    process_subject_information.set_task_instance_id("download_subject_info")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        maxlen=1000,
        output_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        **download_subject_info_params,
    )
    .mapvalues(argnames=["subject_df"], argvalues=split_subject_by_group)
)


# %% [markdown]
# ## Persist subject information as csv

# %%
# parameters

persist_subject_info_params = dict()

# %%
# call the task


persist_subject_info = (
    persist_df.set_task_instance_id("persist_subject_info")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        filetype="csv",
        filename=None,
        **persist_subject_info_params,
    )
    .mapvalues(argnames=["df"], argvalues=download_subject_info)
)


# %% [markdown]
# ## Retrieve events from ER

# %%
# parameters

get_events_data_params = dict()

# %%
# call the task


get_events_data = (
    get_events.set_task_instance_id("get_events_data")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        client=er_client_name,
        time_range=time_range,
        include_details=True,
        raise_on_empty=False,
        include_null_geometry=True,
        include_updates=False,
        include_related_events=False,
        include_display_values=False,
        event_types=["mep_collar_check", "mep_collaring", "mep_source_failure"],
        event_columns=[
            "id",
            "time",
            "event_type",
            "event_category",
            "reported_by",
            "serial_number",
            "geometry",
            "event_details",
            "priority",
            "priority_label",
        ],
        **get_events_data_params,
    )
    .call()
)


# %% [markdown]
# ## Normalize event details

# %%
# parameters

normalize_event_details_params = dict()

# %%
# call the task


normalize_event_details = (
    normalize_json_column.set_task_instance_id("normalize_event_details")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        column="event_details",
        df=get_events_data,
        skip_if_not_exists=True,
        sort_columns=True,
        **normalize_event_details_params,
    )
    .call()
)


# %% [markdown]
# ## Rename event columns

# %%
# parameters

rename_event_cols_params = dict()

# %%
# call the task


rename_event_cols = (
    transform_columns.set_task_instance_id("rename_event_cols")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        drop_columns=[],
        retain_columns=[],
        rename_columns={
            "event_details__pic": "pic",
            "event_details__region": "region",
            "event_details__source": "source",
            "event_details__details": "details",
            "event_details__subject": "groupby_col",
            "event_details__source_id": "source_id",
            "event_details__subject_id": "subject_name",
            "event_details__collaring_type": "collaring_type",
            "event_details__collaring_reason": "collaring_reason",
            "event_details__collar_checked_by": "collar_checked_by",
        },
        skip_missing_rename=True,
        required_columns=["event_details__subject_id"],
        df=normalize_event_details,
        **rename_event_cols_params,
    )
    .call()
)


# %% [markdown]
# ## split events by group

# %%
# parameters

split_events_by_group_params = dict()

# %%
# call the task


split_events_by_group = (
    split_groups.set_task_instance_id("split_events_by_group")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(df=rename_event_cols, groupers=groupers, **split_events_by_group_params)
    .call()
)


# %% [markdown]
# ## Split relocations by group

# %%
# parameters

split_relocs_by_group_params = dict()

# %%
# call the task


split_relocs_by_group = (
    split_groups.set_task_instance_id("split_relocs_by_group")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(df=rename_reloc_cols, groupers=groupers, **split_relocs_by_group_params)
    .call()
)


# %% [markdown]
# ## Trajectory Segment Filter

# %%
# parameters

custom_trajs_filter_params = dict(
    min_length_meters=...,
    max_length_meters=...,
    min_time_secs=...,
    max_time_secs=...,
    min_speed_kmhr=...,
    max_speed_kmhr=...,
)

# %%
# call the task


custom_trajs_filter = (
    custom_trajectory_segment_filter.set_task_instance_id("custom_trajs_filter")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(**custom_trajs_filter_params)
    .call()
)


# %% [markdown]
# ## Convert relocations to trajectories

# %%
# parameters

convert_to_trajectories_params = dict()

# %%
# call the task


convert_to_trajectories = (
    relocations_to_trajectory.set_task_instance_id("convert_to_trajectories")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        relocations=rename_reloc_cols,
        trajectory_segment_filter=custom_trajs_filter,
        **convert_to_trajectories_params,
    )
    .call()
)


# %% [markdown]
# ## Add temporal index to trajectories

# %%
# parameters

add_temporal_index_to_traj_params = dict()

# %%
# call the task


add_temporal_index_to_traj = (
    add_temporal_index.set_task_instance_id("add_temporal_index_to_traj")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        df=convert_to_trajectories,
        time_col="segment_start",
        groupers=groupers,
        cast_to_datetime=True,
        format="mixed",
        **add_temporal_index_to_traj_params,
    )
    .call()
)


# %% [markdown]
# ## Classify trajectories by speed

# %%
# parameters

classify_trajectories_speed_bins_params = dict()

# %%
# call the task


classify_trajectories_speed_bins = (
    apply_classification.set_task_instance_id("classify_trajectories_speed_bins")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        df=add_temporal_index_to_traj,
        input_column_name="speed_kmhr",
        output_column_name="speed_bins",
        classification_options={"scheme": "equal_interval", "k": 6},
        label_options={
            "label_ranges": True,
            "label_decimals": 1,
            "label_suffix": " km/h",
        },
        **classify_trajectories_speed_bins_params,
    )
    .call()
)


# %% [markdown]
# ## Rename trajectory columns

# %%
# parameters

rename_traj_cols_params = dict()

# %%
# call the task


rename_traj_cols = (
    map_columns.set_task_instance_id("rename_traj_cols")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        raise_if_not_found=True,
        df=classify_trajectories_speed_bins,
        drop_columns=[],
        retain_columns=[],
        rename_columns={
            "extra__hex_color": "hex_color",
            "extra__subject_name": "subject_name",
            "extra__subject_sex": "subject_sex",
            "extra__subject_subtype": "subject_subtype",
            "extra__created_at": "created_at",
        },
        **rename_traj_cols_params,
    )
    .call()
)


# %% [markdown]
# ## Split trajectories by group

# %%
# parameters

split_traj_by_group_params = dict()

# %%
# call the task


split_traj_by_group = (
    split_groups.set_task_instance_id("split_traj_by_group")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(df=rename_traj_cols, groupers=groupers, **split_traj_by_group_params)
    .call()
)


# %% [markdown]
# ## Sort trajectories by speed bins

# %%
# parameters

sort_trajs_by_speed_params = dict()

# %%
# call the task


sort_trajs_by_speed = (
    sort_values.set_task_instance_id("sort_trajs_by_speed")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        column_name="speed_bins",
        na_position="first",
        ascending=True,
        **sort_trajs_by_speed_params,
    )
    .mapvalues(argnames=["df"], argvalues=split_traj_by_group)
)


# %% [markdown]
# ## Apply colormap to speed bins

# %%
# parameters

apply_speed_colormap_params = dict()

# %%
# call the task


apply_speed_colormap = (
    apply_color_map.set_task_instance_id("apply_speed_colormap")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        input_column_name="speed_bins",
        output_column_name="speed_bins_colormap",
        colormap=["#1a9850", "#91cf60", "#d9ef8b", "#fee08b", "#fc8d59", "#d73027"],
        **apply_speed_colormap_params,
    )
    .mapvalues(argnames=["df"], argvalues=sort_trajs_by_speed)
)


# %% [markdown]
# ## Filter df for map rendering

# %%
# parameters

filter_speedmap_gdf_params = dict()

# %%
# call the task


filter_speedmap_gdf = (
    filter_df_cols.set_task_instance_id("filter_speedmap_gdf")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        columns=["geometry", "speed_kmhr", "speed_bins", "speed_bins_colormap"],
        **filter_speedmap_gdf_params,
    )
    .mapvalues(argnames=["df"], argvalues=apply_speed_colormap)
)


# %% [markdown]
# ## Generate speedmap layers

# %%
# parameters

generate_speedmap_layers_params = dict()

# %%
# call the task


generate_speedmap_layers = (
    create_path_layer.set_task_instance_id("generate_speedmap_layers")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        layer_style={
            "get_color": "speed_bins_colormap",
            "get_width": 2.55,
            "width_scale": 1,
            "width_min_pixels": 2,
            "width_max_pixels": 8,
            "width_units": "pixels",
            "cap_rounded": True,
            "joint_rounded": True,
            "billboard": False,
            "opacity": 0.45,
            "stroked": True,
        },
        legend={
            "title": "Speed (km/h)",
            "label_column": "speed_bins",
            "color_column": "speed_bins_colormap",
            "sort": "ascending",
            "label_suffix": None,
        },
        **generate_speedmap_layers_params,
    )
    .mapvalues(argnames=["geodataframe"], argvalues=filter_speedmap_gdf)
)


# %% [markdown]
# ## Zoom to gdf extent

# %%
# parameters

gdf_bounding_extent_params = dict(
    expansion_factor=...,
)

# %%
# call the task


gdf_bounding_extent = (
    envelope_gdf.set_task_instance_id("gdf_bounding_extent")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(**gdf_bounding_extent_params)
    .mapvalues(argnames=["gdf"], argvalues=filter_speedmap_gdf)
)


# %% [markdown]
# ## Zoom to gdf extent for image

# %%
# parameters

zoom_gdf_extent_params = dict()

# %%
# call the task


zoom_gdf_extent = (
    view_state_deck_gdf.set_task_instance_id("zoom_gdf_extent")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(pitch=0, bearing=0, **zoom_gdf_extent_params)
    .mapvalues(argnames=["gdf"], argvalues=gdf_bounding_extent)
)


# %% [markdown]
# ## Zoom to gdf extent

# %%
# parameters

zoom_speed_gdf_extent_params = dict()

# %%
# call the task


zoom_speed_gdf_extent = (
    custom_view_state_deck_gdf.set_task_instance_id("zoom_speed_gdf_extent")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        pitch=0,
        bearing=0,
        buffer=0.375,
        map_width_px=900,
        map_height_px=700,
        **zoom_speed_gdf_extent_params,
    )
    .mapvalues(argnames=["gdf"], argvalues=filter_speedmap_gdf)
)


# %% [markdown]
# ## Combine ldx layers and speedmap layers

# %%
# parameters

combined_ldx_speed_layers_params = dict()

# %%
# call the task


combined_ldx_speed_layers = (
    combine_deckgl_map_layers.set_task_instance_id("combined_ldx_speed_layers")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        static_layers=[create_ldx_styled_layers, create_ldx_text_layer],
        **combined_ldx_speed_layers_params,
    )
    .mapvalues(argnames=["grouped_layers"], argvalues=generate_speedmap_layers)
)


# %% [markdown]
# ## Combine speedmap layers with view state

# %%
# parameters

zip_speedmap_with_viewstate_params = dict()

# %%
# call the task


zip_speedmap_with_viewstate = (
    zip_groupbykey.set_task_instance_id("zip_speedmap_with_viewstate")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        sequences=[combined_ldx_speed_layers, zoom_gdf_extent],
        **zip_speedmap_with_viewstate_params,
    )
    .call()
)


# %% [markdown]
# ## Draw speedmap

# %%
# parameters

draw_speedmap_params = dict(
    widget_id=...,
)

# %%
# call the task


draw_speedmap = (
    draw_map.set_task_instance_id("draw_speedmap")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        tile_layers=configure_base_maps,
        static=False,
        title=None,
        max_zoom=10,
        legend_style={"placement": "bottom-right"},
        **draw_speedmap_params,
    )
    .mapvalues(
        argnames=["geo_layers", "view_state"], argvalues=zip_speedmap_with_viewstate
    )
)


# %% [markdown]
# ## Persist speedmap html

# %%
# parameters

persist_speedmap_html_params = dict(
    filename=...,
)

# %%
# call the task


persist_speedmap_html = (
    persist_text.set_task_instance_id("persist_speedmap_html")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        filename_suffix="speedmap",
        **persist_speedmap_html_params,
    )
    .mapvalues(argnames=["text"], argvalues=draw_speedmap)
)


# %% [markdown]
# ## Create speedmap widgets

# %%
# parameters

create_speedmap_widgets_params = dict()

# %%
# call the task


create_speedmap_widgets = (
    create_map_widget_single_view.set_task_instance_id("create_speedmap_widgets")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(title="Speed Map", **create_speedmap_widgets_params)
    .map(argnames=["view", "data"], argvalues=persist_speedmap_html)
)


# %% [markdown]
# ## Merge speedmap widgets

# %%
# parameters

merge_speedmap_widgets_params = dict()

# %%
# call the task


merge_speedmap_widgets = (
    merge_widget_views.set_task_instance_id("merge_speedmap_widgets")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(widgets=create_speedmap_widgets, **merge_speedmap_widgets_params)
    .call()
)


# %% [markdown]
# ## Generate etd

# %%
# parameters

generate_etd_params = dict()

# %%
# call the task


generate_etd = (
    calculate_elliptical_time_density.set_task_instance_id("generate_etd")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        auto_scale_or_custom_cell_size={"auto_scale_or_custom": "Auto-scale"},
        crs="ESRI:53042",
        percentiles=[50.0, 60.0, 70.0, 80.0, 90.0, 95.0, 99.9],
        nodata_value="nan",
        band_count=1,
        max_speed_factor=1.05,
        expansion_factor=1.3,
        **generate_etd_params,
    )
    .mapvalues(argnames=["trajectory_gdf"], argvalues=split_traj_by_group)
)


# %% [markdown]
# ## Determine seasonal windows

# %%
# parameters

determine_seasonal_windows_params = dict()

# %%
# call the task


determine_seasonal_windows = (
    determine_season_windows.set_task_instance_id("determine_seasonal_windows")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        client=gee_project_name,
        time_range=time_range,
        **determine_seasonal_windows_params,
    )
    .mapvalues(argnames=["roi"], argvalues=generate_etd)
)


# %% [markdown]
# ## Persist subject seasonal windows

# %%
# parameters

persist_subject_seasonal_windows_params = dict()

# %%
# call the task


persist_subject_seasonal_windows = (
    persist_df.set_task_instance_id("persist_subject_seasonal_windows")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        filetype="csv",
        filename=None,
        **persist_subject_seasonal_windows_params,
    )
    .mapvalues(argnames=["df"], argvalues=determine_seasonal_windows)
)


# %% [markdown]
# ## Zip seasons with trajectories

# %%
# parameters

zip_etd_with_traj_params = dict()

# %%
# call the task


zip_etd_with_traj = (
    zip_groupbykey.set_task_instance_id("zip_etd_with_traj")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        sequences=[determine_seasonal_windows, split_traj_by_group],
        **zip_etd_with_traj_params,
    )
    .call()
)


# %% [markdown]
# ## Generate season labels

# %%
# parameters

add_season_labels_params = dict()

# %%
# call the task


add_season_labels = (
    create_seasonal_labels.set_task_instance_id("add_season_labels")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(**add_season_labels_params)
    .mapvalues(argnames=["seasons_df", "trajectories"], argvalues=zip_etd_with_traj)
)


# %% [markdown]
# ## Generate MCP gdf from trajectories

# %%
# parameters

generate_mcp_params = dict()

# %%
# call the task


generate_mcp = (
    generate_mcp_gdf.set_task_instance_id("generate_mcp")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(planar_crs="ESRI:53042", **generate_mcp_params)
    .mapvalues(argnames=["gdf"], argvalues=split_traj_by_group)
)


# %% [markdown]
# ## Apply colormap to home range percentiles

# %%
# parameters

apply_etd_colormap_params = dict()

# %%
# call the task


apply_etd_colormap = (
    apply_color_map.set_task_instance_id("apply_etd_colormap")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        input_column_name="percentile",
        output_column_name="etd_percentile_colors",
        colormap="RdYlGn",
        **apply_etd_colormap_params,
    )
    .mapvalues(argnames=["df"], argvalues=generate_etd)
)


# %% [markdown]
# ## Create home range layers

# %%
# parameters

generate_home_range_layers_params = dict()

# %%
# call the task


generate_home_range_layers = (
    create_geojson_layer.set_task_instance_id("generate_home_range_layers")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        layer_style={
            "filled": True,
            "stroked": True,
            "extruded": False,
            "wireframe": False,
            "get_fill_color": "etd_percentile_colors",
            "get_line_color": "etd_percentile_colors",
            "opacity": 0.45,
            "get_line_width": 1.55,
            "get_elevation": 0,
            "get_point_radius": 1,
            "line_width_units": "pixels",
            "line_width_scale": 1,
            "line_width_min_pixels": 1,
            "line_width_max_pixels": 5,
        },
        legend={
            "title": "Home Range Percentiles",
            "label_column": "percentile",
            "color_column": "etd_percentile_colors",
            "sort": "ascending",
            "label_suffix": None,
        },
        **generate_home_range_layers_params,
    )
    .mapvalues(argnames=["geodataframe"], argvalues=apply_etd_colormap)
)


# %% [markdown]
# ## Combine ldx layers and home range layers

# %%
# parameters

combined_ldx_home_range_layers_params = dict()

# %%
# call the task


combined_ldx_home_range_layers = (
    combine_deckgl_map_layers.set_task_instance_id("combined_ldx_home_range_layers")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        static_layers=[create_ldx_styled_layers, create_ldx_text_layer],
        **combined_ldx_home_range_layers_params,
    )
    .mapvalues(argnames=["grouped_layers"], argvalues=generate_home_range_layers)
)


# %% [markdown]
# ## Zoom to  home range gdf extent

# %%
# parameters

zoom_hr_gdf_extent_params = dict()

# %%
# call the task


zoom_hr_gdf_extent = (
    custom_view_state_deck_gdf.set_task_instance_id("zoom_hr_gdf_extent")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        pitch=0,
        bearing=0,
        buffer=0.375,
        map_width_px=602,
        map_height_px=855,
        **zoom_hr_gdf_extent_params,
    )
    .mapvalues(argnames=["gdf"], argvalues=apply_etd_colormap)
)


# %% [markdown]
# ## Combine home range layers with view state

# %%
# parameters

zip_hr_with_viewstate_params = dict()

# %%
# call the task


zip_hr_with_viewstate = (
    zip_groupbykey.set_task_instance_id("zip_hr_with_viewstate")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        sequences=[combined_ldx_home_range_layers, zoom_gdf_extent],
        **zip_hr_with_viewstate_params,
    )
    .call()
)


# %% [markdown]
# ## Draw home range map

# %%
# parameters

draw_home_range_map_params = dict(
    widget_id=...,
)

# %%
# call the task


draw_home_range_map = (
    draw_map.set_task_instance_id("draw_home_range_map")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        tile_layers=configure_base_maps,
        static=False,
        title=None,
        max_zoom=10,
        legend_style={"placement": "bottom-right"},
        **draw_home_range_map_params,
    )
    .mapvalues(argnames=["geo_layers", "view_state"], argvalues=zip_hr_with_viewstate)
)


# %% [markdown]
# ## Persist homerange html

# %%
# parameters

persist_homerange_html_params = dict(
    filename=...,
)

# %%
# call the task


persist_homerange_html = (
    persist_text.set_task_instance_id("persist_homerange_html")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        filename_suffix="homerange",
        **persist_homerange_html_params,
    )
    .mapvalues(argnames=["text"], argvalues=draw_home_range_map)
)


# %% [markdown]
# ## Create homerange widgets

# %%
# parameters

create_home_range_widgets_params = dict()

# %%
# call the task


create_home_range_widgets = (
    create_map_widget_single_view.set_task_instance_id("create_home_range_widgets")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(title="Home Range", **create_home_range_widgets_params)
    .map(argnames=["view", "data"], argvalues=persist_homerange_html)
)


# %% [markdown]
# ## Merge homerange widgets

# %%
# parameters

merge_homerange_widgets_params = dict()

# %%
# call the task


merge_homerange_widgets = (
    merge_widget_views.set_task_instance_id("merge_homerange_widgets")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(widgets=create_home_range_widgets, **merge_homerange_widgets_params)
    .call()
)


# %% [markdown]
# ## Calculate seasonal home range

# %%
# parameters

seasonal_home_range_params = dict()

# %%
# call the task


seasonal_home_range = (
    calculate_seasonal_home_range.set_task_instance_id("seasonal_home_range")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        groupby_cols=["season"],
        percentiles=[99.9],
        auto_scale_or_custom_cell_size={"auto_scale_or_custom": "Auto-scale"},
        **seasonal_home_range_params,
    )
    .mapvalues(argnames=["gdf"], argvalues=add_season_labels)
)


# %% [markdown]
# ## Convert season column to string

# %%
# parameters

convert_season_to_string_params = dict()

# %%
# call the task


convert_season_to_string = (
    convert_to_str.set_task_instance_id("convert_season_to_string")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(columns=["season"], **convert_season_to_string_params)
    .mapvalues(argnames=["df"], argvalues=seasonal_home_range)
)


# %% [markdown]
# ## Apply colormap to seasonal home range

# %%
# parameters

apply_seasonal_colormap_params = dict()

# %%
# call the task


apply_seasonal_colormap = (
    apply_color_map.set_task_instance_id("apply_seasonal_colormap")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        input_column_name="season",
        output_column_name="season_colors",
        colormap=["#00bfff", "#ff7f50"],
        **apply_seasonal_colormap_params,
    )
    .mapvalues(argnames=["df"], argvalues=convert_season_to_string)
)


# %% [markdown]
# ## Create seasonal home range layers

# %%
# parameters

generate_season_layers_params = dict()

# %%
# call the task


generate_season_layers = (
    create_geojson_layer.set_task_instance_id("generate_season_layers")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        layer_style={
            "filled": True,
            "stroked": True,
            "extruded": False,
            "wireframe": False,
            "get_fill_color": "season_colors",
            "get_line_color": [0, 0, 0, 255],
            "opacity": 0.15,
            "get_line_width": 0.35,
            "get_elevation": 0,
            "get_point_radius": 1,
            "line_width_units": "pixels",
            "line_width_scale": 1,
            "line_width_min_pixels": 1,
            "line_width_max_pixels": 5,
        },
        legend={
            "title": "Seasonal Home Range",
            "label_column": "season",
            "color_column": "season_colors",
            "sort": "ascending",
            "label_suffix": None,
        },
        **generate_season_layers_params,
    )
    .mapvalues(argnames=["geodataframe"], argvalues=apply_seasonal_colormap)
)


# %% [markdown]
# ## Create mcp polygon layer

# %%
# parameters

create_mcp_polygon_layer_params = dict()

# %%
# call the task


create_mcp_polygon_layer = (
    create_geojson_layer.set_task_instance_id("create_mcp_polygon_layer")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        layer_style={
            "filled": False,
            "stroked": True,
            "extruded": False,
            "wireframe": False,
            "get_fill_color": [255, 20, 147, 50],
            "get_line_color": [255, 20, 147, 200],
            "opacity": 0.35,
            "get_line_width": 1.75,
            "get_elevation": 0,
            "get_point_radius": 1,
            "line_width_units": "pixels",
            "line_width_scale": 1,
            "line_width_min_pixels": 1,
            "line_width_max_pixels": 5,
        },
        legend={"title": "", "values": [{"label": "MCP", "color": "#ff1493"}]},
        **create_mcp_polygon_layer_params,
    )
    .mapvalues(argnames=["geodataframe"], argvalues=generate_mcp)
)


# %% [markdown]
# ## Combine seasonal home range layer and mcp layer

# %%
# parameters

zip_season_mcp_layer_params = dict()

# %%
# call the task


zip_season_mcp_layer = (
    zip_groupbykey.set_task_instance_id("zip_season_mcp_layer")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        sequences=[create_mcp_polygon_layer, generate_season_layers],
        **zip_season_mcp_layer_params,
    )
    .call()
)


# %% [markdown]
# ## Combine ldx layers and seasonal home range layers

# %%
# parameters

combined_ldx_seasonal_hr_layers_params = dict()

# %%
# call the task


combined_ldx_seasonal_hr_layers = (
    combine_deckgl_map_layers.set_task_instance_id("combined_ldx_seasonal_hr_layers")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        static_layers=[create_ldx_styled_layers, create_ldx_text_layer],
        **combined_ldx_seasonal_hr_layers_params,
    )
    .mapvalues(argnames=["grouped_layers"], argvalues=zip_season_mcp_layer)
)


# %% [markdown]
# ## Zoom to seasons gdf extent

# %%
# parameters

zoom_seasons_gdf_extent_params = dict()

# %%
# call the task


zoom_seasons_gdf_extent = (
    custom_view_state_deck_gdf.set_task_instance_id("zoom_seasons_gdf_extent")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        pitch=0,
        bearing=0,
        buffer=0.375,
        map_width_px=602,
        map_height_px=855,
        **zoom_seasons_gdf_extent_params,
    )
    .mapvalues(argnames=["gdf"], argvalues=apply_seasonal_colormap)
)


# %% [markdown]
# ## Combine seasonal home range layers with view state

# %%
# parameters

zip_seasonal_hr_with_viewstate_params = dict()

# %%
# call the task


zip_seasonal_hr_with_viewstate = (
    zip_groupbykey.set_task_instance_id("zip_seasonal_hr_with_viewstate")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        sequences=[combined_ldx_seasonal_hr_layers, zoom_gdf_extent],
        **zip_seasonal_hr_with_viewstate_params,
    )
    .call()
)


# %% [markdown]
# ## Draw seasonal home range map

# %%
# parameters

draw_seasonal_home_range_map_params = dict(
    widget_id=...,
)

# %%
# call the task


draw_seasonal_home_range_map = (
    draw_map.set_task_instance_id("draw_seasonal_home_range_map")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        tile_layers=configure_base_maps,
        static=False,
        title=None,
        max_zoom=10,
        legend_style={"placement": "bottom-right"},
        **draw_seasonal_home_range_map_params,
    )
    .mapvalues(
        argnames=["geo_layers", "view_state"], argvalues=zip_seasonal_hr_with_viewstate
    )
)


# %% [markdown]
# ## Persist seasonal home range html

# %%
# parameters

persist_seasonal_home_range_html_params = dict(
    filename=...,
)

# %%
# call the task


persist_seasonal_home_range_html = (
    persist_text.set_task_instance_id("persist_seasonal_home_range_html")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        filename_suffix="seasonal_home_range",
        **persist_seasonal_home_range_html_params,
    )
    .mapvalues(argnames=["text"], argvalues=draw_seasonal_home_range_map)
)


# %% [markdown]
# ## Create seasonal home range widgets

# %%
# parameters

create_seasonal_hr_widgets_params = dict()

# %%
# call the task


create_seasonal_hr_widgets = (
    create_map_widget_single_view.set_task_instance_id("create_seasonal_hr_widgets")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(title="Seasonal Home Range", **create_seasonal_hr_widgets_params)
    .map(argnames=["view", "data"], argvalues=persist_seasonal_home_range_html)
)


# %% [markdown]
# ## Merge seasonal home range widgets

# %%
# parameters

merge_seasonal_hr_widgets_params = dict()

# %%
# call the task


merge_seasonal_hr_widgets = (
    merge_widget_views.set_task_instance_id("merge_seasonal_hr_widgets")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(widgets=create_seasonal_hr_widgets, **merge_seasonal_hr_widgets_params)
    .call()
)


# %% [markdown]
# ## Zip relocations gdf with seasonal windows

# %%
# parameters

zip_relocs_with_seasons_params = dict()

# %%
# call the task


zip_relocs_with_seasons = (
    zip_groupbykey.set_task_instance_id("zip_relocs_with_seasons")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        sequences=[determine_seasonal_windows, split_relocs_by_group],
        **zip_relocs_with_seasons_params,
    )
    .call()
)


# %% [markdown]
# ## Generate NSD seasonal plot

# %%
# parameters

generate_nsd_seasonal_plot_params = dict(
    widget_id=...,
)

# %%
# call the task


generate_nsd_seasonal_plot = (
    draw_season_nsd_plot.set_task_instance_id("generate_nsd_seasonal_plot")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(**generate_nsd_seasonal_plot_params)
    .mapvalues(
        argnames=["seasons_df", "relocations_gdf"], argvalues=zip_relocs_with_seasons
    )
)


# %% [markdown]
# ## Persist seasonal nsd plots

# %%
# parameters

persist_nsd_html_urls_params = dict(
    filename=...,
)

# %%
# call the task


persist_nsd_html_urls = (
    persist_text.set_task_instance_id("persist_nsd_html_urls")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        filename_suffix="nsd_seasonal_plot",
        **persist_nsd_html_urls_params,
    )
    .mapvalues(argnames=["text"], argvalues=generate_nsd_seasonal_plot)
)


# %% [markdown]
# ## Create widget for nsd plot

# %%
# parameters

nsd_plot_widgets_single_view_params = dict()

# %%
# call the task


nsd_plot_widgets_single_view = (
    create_map_widget_single_view.set_task_instance_id("nsd_plot_widgets_single_view")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            never,
        ],
        unpack_depth=1,
    )
    .partial(
        title="Net Square Displacement (NSD)", **nsd_plot_widgets_single_view_params
    )
    .map(argnames=["view", "data"], argvalues=persist_nsd_html_urls)
)


# %% [markdown]
# ## Merge nsd plot widgets

# %%
# parameters

grouped_nsd_plot_widget_params = dict()

# %%
# call the task


grouped_nsd_plot_widget = (
    merge_widget_views.set_task_instance_id("grouped_nsd_plot_widget")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(widgets=nsd_plot_widgets_single_view, **grouped_nsd_plot_widget_params)
    .call()
)


# %% [markdown]
# ## Generate speed seasonal plot

# %%
# parameters

generate_speed_seasonal_plot_params = dict(
    widget_id=...,
)

# %%
# call the task


generate_speed_seasonal_plot = (
    draw_season_speed_plot.set_task_instance_id("generate_speed_seasonal_plot")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(**generate_speed_seasonal_plot_params)
    .mapvalues(
        argnames=["seasons_df", "relocations_gdf"], argvalues=zip_relocs_with_seasons
    )
)


# %% [markdown]
# ## Persist seasonal speed plots

# %%
# parameters

persist_speed_html_urls_params = dict(
    filename=...,
)

# %%
# call the task


persist_speed_html_urls = (
    persist_text.set_task_instance_id("persist_speed_html_urls")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        filename_suffix="speed_seasonal_plot",
        **persist_speed_html_urls_params,
    )
    .mapvalues(argnames=["text"], argvalues=generate_speed_seasonal_plot)
)


# %% [markdown]
# ## Create widget for speed plot

# %%
# parameters

speed_plot_widgets_single_view_params = dict()

# %%
# call the task


speed_plot_widgets_single_view = (
    create_map_widget_single_view.set_task_instance_id("speed_plot_widgets_single_view")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            never,
        ],
        unpack_depth=1,
    )
    .partial(title="Speed", **speed_plot_widgets_single_view_params)
    .map(argnames=["view", "data"], argvalues=persist_speed_html_urls)
)


# %% [markdown]
# ## Merge speed plot widgets

# %%
# parameters

grouped_speed_plot_widget_params = dict()

# %%
# call the task


grouped_speed_plot_widget = (
    merge_widget_views.set_task_instance_id("grouped_speed_plot_widget")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(widgets=speed_plot_widgets_single_view, **grouped_speed_plot_widget_params)
    .call()
)


# %% [markdown]
# ## Generate collared subject plot

# %%
# parameters

generate_collared_subject_plot_params = dict(
    widget_id=...,
)

# %%
# call the task


generate_collared_subject_plot = (
    draw_season_collared_plot.set_task_instance_id("generate_collared_subject_plot")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        events_gdf=rename_event_cols,
        filter_col="subject_name",
        **generate_collared_subject_plot_params,
    )
    .mapvalues(
        argnames=["seasons_df", "relocations_gdf"], argvalues=zip_relocs_with_seasons
    )
)


# %% [markdown]
# ## Persist collared subject plots

# %%
# parameters

persist_collared_subject_plots_params = dict(
    filename=...,
)

# %%
# call the task


persist_collared_subject_plots = (
    persist_text.set_task_instance_id("persist_collared_subject_plots")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        filename_suffix="collared_subject_plot",
        **persist_collared_subject_plots_params,
    )
    .mapvalues(argnames=["text"], argvalues=generate_collared_subject_plot)
)


# %% [markdown]
# ## Create widget for collared subject plot

# %%
# parameters

collared_widget_view_params = dict()

# %%
# call the task


collared_widget_view = (
    create_map_widget_single_view.set_task_instance_id("collared_widget_view")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            never,
        ],
        unpack_depth=1,
    )
    .partial(title="Collared Subject Plot", **collared_widget_view_params)
    .map(argnames=["view", "data"], argvalues=persist_collared_subject_plots)
)


# %% [markdown]
# ## Merge collared plot widgets

# %%
# parameters

grouped_collared_widget_params = dict()

# %%
# call the task


grouped_collared_widget = (
    merge_widget_views.set_task_instance_id("grouped_collared_widget")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(widgets=collared_widget_view, **grouped_collared_widget_params)
    .call()
)


# %% [markdown]
# ## Generate mcp asymptote plot

# %%
# parameters

generate_mcp_asymp_plot_params = dict(
    widget_id=...,
)

# %%
# call the task


generate_mcp_asymp_plot = (
    draw_season_mcp_plot.set_task_instance_id("generate_mcp_asymp_plot")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(**generate_mcp_asymp_plot_params)
    .mapvalues(
        argnames=["seasons_df", "relocations_gdf"], argvalues=zip_relocs_with_seasons
    )
)


# %% [markdown]
# ## Persist mcp asymptote plots

# %%
# parameters

persist_mcp_html_urls_params = dict(
    filename=...,
)

# %%
# call the task


persist_mcp_html_urls = (
    persist_text.set_task_instance_id("persist_mcp_html_urls")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        filename_suffix="mcp_asymptote_plot",
        **persist_mcp_html_urls_params,
    )
    .mapvalues(argnames=["text"], argvalues=generate_mcp_asymp_plot)
)


# %% [markdown]
# ## Create widget for mcp plot

# %%
# parameters

mcp_plot_widgets_single_view_params = dict()

# %%
# call the task


mcp_plot_widgets_single_view = (
    create_map_widget_single_view.set_task_instance_id("mcp_plot_widgets_single_view")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            never,
        ],
        unpack_depth=1,
    )
    .partial(title="MCP Asymptote", **mcp_plot_widgets_single_view_params)
    .map(argnames=["view", "data"], argvalues=persist_mcp_html_urls)
)


# %% [markdown]
# ## Merge mcp plot widgets

# %%
# parameters

grouped_mcp_plot_widget_params = dict()

# %%
# call the task


grouped_mcp_plot_widget = (
    merge_widget_views.set_task_instance_id("grouped_mcp_plot_widget")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(widgets=mcp_plot_widgets_single_view, **grouped_mcp_plot_widget_params)
    .call()
)


# %% [markdown]
# ## zip trajectories gdf with etd gdf

# %%
# parameters

zip_traj_etd_gdf_params = dict()

# %%
# call the task


zip_traj_etd_gdf = (
    zip_groupbykey.set_task_instance_id("zip_traj_etd_gdf")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(sequences=[generate_etd, split_traj_by_group], **zip_traj_etd_gdf_params)
    .call()
)


# %% [markdown]
# ## Generate subject stats

# %%
# parameters

generate_subject_stats_params = dict()

# %%
# call the task


generate_subject_stats = (
    compute_subject_stats.set_task_instance_id("generate_subject_stats")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        output_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        subject_df=compute_subject_maturity,
        groupby_col="subject_name",
        **generate_subject_stats_params,
    )
    .mapvalues(argnames=["etd_df", "traj_gdf"], argvalues=zip_traj_etd_gdf)
)


# %% [markdown]
# ## Persist subject stats as csv

# %%
# parameters

persist_subject_stats_params = dict()

# %%
# call the task


persist_subject_stats = (
    persist_df.set_task_instance_id("persist_subject_stats")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        filetype="csv",
        filename=None,
        **persist_subject_stats_params,
    )
    .mapvalues(argnames=["df"], argvalues=generate_subject_stats)
)


# %% [markdown]
# ## zip etd df with subject df

# %%
# parameters

zip_etd_subject_df_params = dict()

# %%
# call the task


zip_etd_subject_df = (
    zip_groupbykey.set_task_instance_id("zip_etd_subject_df")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        sequences=[generate_etd, split_subject_by_group], **zip_etd_subject_df_params
    )
    .call()
)


# %% [markdown]
# ## Build regional lookup from ldx db

# %%
# parameters

build_regional_lookup_params = dict()

# %%
# call the task


build_regional_lookup = (
    build_template_region_lookup.set_task_instance_id("build_regional_lookup")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        gdf=load_ldx, categories=None, static_ids=None, **build_regional_lookup_params
    )
    .call()
)


# %% [markdown]
# ## Compute template regions

# %%
# parameters

comp_template_regions_params = dict()

# %%
# call the task


comp_template_regions = (
    compute_template_regions.set_task_instance_id("comp_template_regions")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        geodataframe=load_ldx,
        template_lookup=build_regional_lookup,
        crs="ESRI:53042",
        **comp_template_regions_params,
    )
    .call()
)


# %% [markdown]
# ## Calculate subject occupancy

# %%
# parameters

process_subject_occupancy_params = dict()

# %%
# call the task


process_subject_occupancy = (
    compute_subject_occupancy.set_task_instance_id("process_subject_occupancy")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        crs="ESRI:53042",
        regions_gdf=comp_template_regions,
        output_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        **process_subject_occupancy_params,
    )
    .mapvalues(argnames=["etd_gdf", "subjects_df"], argvalues=zip_etd_subject_df)
)


# %% [markdown]
# ## persist subject occupancy as csv

# %%
# parameters

persist_subject_occupancy_params = dict()

# %%
# call the task


persist_subject_occupancy = (
    persist_df.set_task_instance_id("persist_subject_occupancy")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        filetype="csv",
        filename=None,
        **persist_subject_occupancy_params,
    )
    .mapvalues(argnames=["df"], argvalues=process_subject_occupancy)
)


# %% [markdown]
# ## Calculate total national protected area use

# %%
# parameters

total_national_pa_use_params = dict()

# %%
# call the task


total_national_pa_use = (
    dataframe_column_sum.set_task_instance_id("total_national_pa_use")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(column_name="national_pa_use", **total_national_pa_use_params)
    .mapvalues(argnames=["df"], argvalues=process_subject_occupancy)
)


# %% [markdown]
# ## Convert total national protected area to quantity

# %%
# parameters

national_pa_quantity_params = dict()

# %%
# call the task


national_pa_quantity = (
    to_quantity.set_task_instance_id("national_pa_quantity")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(unit="%", **national_pa_quantity_params)
    .mapvalues(argnames=["value"], argvalues=total_national_pa_use)
)


# %% [markdown]
# ## Create single value widgets for national protected area per group

# %%
# parameters

total_pa_sv_widgets_params = dict()

# %%
# call the task


total_pa_sv_widgets = (
    create_single_value_widget_single_view.set_task_instance_id("total_pa_sv_widgets")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            never,
        ],
        unpack_depth=1,
    )
    .partial(
        title="National protected area use",
        decimal_places=2,
        **total_pa_sv_widgets_params,
    )
    .map(argnames=["view", "data"], argvalues=national_pa_quantity)
)


# %% [markdown]
# ## Merge per group total national protected area SV widgets

# %%
# parameters

national_pa_grouped_sv_widget_params = dict()

# %%
# call the task


national_pa_grouped_sv_widget = (
    merge_widget_views.set_task_instance_id("national_pa_grouped_sv_widget")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(widgets=total_pa_sv_widgets, **national_pa_grouped_sv_widget_params)
    .call()
)


# %% [markdown]
# ## Calculate total community protected area use

# %%
# parameters

total_community_pa_use_params = dict()

# %%
# call the task


total_community_pa_use = (
    dataframe_column_sum.set_task_instance_id("total_community_pa_use")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(column_name="community_pa_use", **total_community_pa_use_params)
    .mapvalues(argnames=["df"], argvalues=process_subject_occupancy)
)


# %% [markdown]
# ## Convert total community protected area to quantity

# %%
# parameters

community_pa_quantity_params = dict()

# %%
# call the task


community_pa_quantity = (
    to_quantity.set_task_instance_id("community_pa_quantity")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(unit="%", **community_pa_quantity_params)
    .mapvalues(argnames=["value"], argvalues=total_community_pa_use)
)


# %% [markdown]
# ## Create single value widgets for community protected area per group

# %%
# parameters

total_community_pa_sv_widgets_params = dict()

# %%
# call the task


total_community_pa_sv_widgets = (
    create_single_value_widget_single_view.set_task_instance_id(
        "total_community_pa_sv_widgets"
    )
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            never,
        ],
        unpack_depth=1,
    )
    .partial(
        title="Community protected area use",
        decimal_places=2,
        **total_community_pa_sv_widgets_params,
    )
    .map(argnames=["view", "data"], argvalues=community_pa_quantity)
)


# %% [markdown]
# ## Merge per group total community protected area SV widgets

# %%
# parameters

community_pa_grouped_sv_widget_params = dict()

# %%
# call the task


community_pa_grouped_sv_widget = (
    merge_widget_views.set_task_instance_id("community_pa_grouped_sv_widget")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        widgets=total_community_pa_sv_widgets, **community_pa_grouped_sv_widget_params
    )
    .call()
)


# %% [markdown]
# ## Calculate total crop raid percent

# %%
# parameters

total_crop_raid_percent_params = dict()

# %%
# call the task


total_crop_raid_percent = (
    dataframe_column_sum.set_task_instance_id("total_crop_raid_percent")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(column_name="crop_raid_percent", **total_crop_raid_percent_params)
    .mapvalues(argnames=["df"], argvalues=process_subject_occupancy)
)


# %% [markdown]
# ## Convert total crop raid percent to quantity

# %%
# parameters

crop_raid_percent_quantity_params = dict()

# %%
# call the task


crop_raid_percent_quantity = (
    to_quantity.set_task_instance_id("crop_raid_percent_quantity")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(unit="%", **crop_raid_percent_quantity_params)
    .mapvalues(argnames=["value"], argvalues=total_crop_raid_percent)
)


# %% [markdown]
# ## Create single value widgets for crop raid percent per group

# %%
# parameters

total_crop_raid_sv_widgets_params = dict()

# %%
# call the task


total_crop_raid_sv_widgets = (
    create_single_value_widget_single_view.set_task_instance_id(
        "total_crop_raid_sv_widgets"
    )
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            never,
        ],
        unpack_depth=1,
    )
    .partial(
        title="Agricultural land use",
        decimal_places=2,
        **total_crop_raid_sv_widgets_params,
    )
    .map(argnames=["view", "data"], argvalues=crop_raid_percent_quantity)
)


# %% [markdown]
# ## Merge per group total crop raid percent SV widgets

# %%
# parameters

crop_raid_sv_widget_params = dict()

# %%
# call the task


crop_raid_sv_widget = (
    merge_widget_views.set_task_instance_id("crop_raid_sv_widget")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(widgets=total_crop_raid_sv_widgets, **crop_raid_sv_widget_params)
    .call()
)


# %% [markdown]
# ## Calculate total kenya use

# %%
# parameters

total_kenya_use_params = dict()

# %%
# call the task


total_kenya_use = (
    dataframe_column_sum.set_task_instance_id("total_kenya_use")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(column_name="kenya_use", **total_kenya_use_params)
    .mapvalues(argnames=["df"], argvalues=process_subject_occupancy)
)


# %% [markdown]
# ## Convert total kenya use to quantity

# %%
# parameters

kenya_use_quantity_params = dict()

# %%
# call the task


kenya_use_quantity = (
    to_quantity.set_task_instance_id("kenya_use_quantity")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(unit="%", **kenya_use_quantity_params)
    .mapvalues(argnames=["value"], argvalues=total_kenya_use)
)


# %% [markdown]
# ## Create single value widgets for kenya use per group

# %%
# parameters

total_kenya_use_sv_widgets_params = dict()

# %%
# call the task


total_kenya_use_sv_widgets = (
    create_single_value_widget_single_view.set_task_instance_id(
        "total_kenya_use_sv_widgets"
    )
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            never,
        ],
        unpack_depth=1,
    )
    .partial(title="Kenya use", decimal_places=2, **total_kenya_use_sv_widgets_params)
    .map(argnames=["view", "data"], argvalues=kenya_use_quantity)
)


# %% [markdown]
# ## Merge per group total kenya use SV widgets

# %%
# parameters

kenya_use_grouped_sv_widget_params = dict()

# %%
# call the task


kenya_use_grouped_sv_widget = (
    merge_widget_views.set_task_instance_id("kenya_use_grouped_sv_widget")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(widgets=total_kenya_use_sv_widgets, **kenya_use_grouped_sv_widget_params)
    .call()
)


# %% [markdown]
# ## Calculate total unprotected use

# %%
# parameters

total_unprotected_use_params = dict()

# %%
# call the task


total_unprotected_use = (
    dataframe_column_sum.set_task_instance_id("total_unprotected_use")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(column_name="unprotected", **total_unprotected_use_params)
    .mapvalues(argnames=["df"], argvalues=process_subject_occupancy)
)


# %% [markdown]
# ## Convert total unprotected to quantity

# %%
# parameters

unprotected_quantity_params = dict()

# %%
# call the task


unprotected_quantity = (
    to_quantity.set_task_instance_id("unprotected_quantity")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(unit="%", **unprotected_quantity_params)
    .mapvalues(argnames=["value"], argvalues=total_unprotected_use)
)


# %% [markdown]
# ## Create single value widgets for unprotected per group

# %%
# parameters

total_unprotected_sv_widgets_params = dict()

# %%
# call the task


total_unprotected_sv_widgets = (
    create_single_value_widget_single_view.set_task_instance_id(
        "total_unprotected_sv_widgets"
    )
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            never,
        ],
        unpack_depth=1,
    )
    .partial(
        title="Unprotected use", decimal_places=2, **total_unprotected_sv_widgets_params
    )
    .map(argnames=["view", "data"], argvalues=unprotected_quantity)
)


# %% [markdown]
# ## Merge per group total unprotected SV widgets

# %%
# parameters

unprotected_grouped_sv_widget_params = dict()

# %%
# call the task


unprotected_grouped_sv_widget = (
    merge_widget_views.set_task_instance_id("unprotected_grouped_sv_widget")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        widgets=total_unprotected_sv_widgets, **unprotected_grouped_sv_widget_params
    )
    .call()
)


# %% [markdown]
# ## Calculate subject MCP

# %%
# parameters

compute_subject_mcp_params = dict()

# %%
# call the task


compute_subject_mcp = (
    dataframe_column_sum.set_task_instance_id("compute_subject_mcp")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(column_name="MCP", **compute_subject_mcp_params)
    .mapvalues(argnames=["df"], argvalues=generate_subject_stats)
)


# %% [markdown]
# ## Convert subject mcp to quantity

# %%
# parameters

subject_mcp_quantity_params = dict()

# %%
# call the task


subject_mcp_quantity = (
    to_quantity.set_task_instance_id("subject_mcp_quantity")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(unit="km", **subject_mcp_quantity_params)
    .mapvalues(argnames=["value"], argvalues=compute_subject_mcp)
)


# %% [markdown]
# ## Create single value widgets for subject mcp area per group

# %%
# parameters

total_mcp_sv_widgets_params = dict()

# %%
# call the task


total_mcp_sv_widgets = (
    create_single_value_widget_single_view.set_task_instance_id("total_mcp_sv_widgets")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            never,
        ],
        unpack_depth=1,
    )
    .partial(title="MCP Area", decimal_places=2, **total_mcp_sv_widgets_params)
    .map(argnames=["view", "data"], argvalues=subject_mcp_quantity)
)


# %% [markdown]
# ## Merge per group total subject mcp area SV widgets

# %%
# parameters

subject_mcp_grouped_sv_widget_params = dict()

# %%
# call the task


subject_mcp_grouped_sv_widget = (
    merge_widget_views.set_task_instance_id("subject_mcp_grouped_sv_widget")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(widgets=total_mcp_sv_widgets, **subject_mcp_grouped_sv_widget_params)
    .call()
)


# %% [markdown]
# ## Calculate subject ETD

# %%
# parameters

compute_subject_etd_params = dict()

# %%
# call the task


compute_subject_etd = (
    dataframe_column_sum.set_task_instance_id("compute_subject_etd")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(column_name="ETD", **compute_subject_etd_params)
    .mapvalues(argnames=["df"], argvalues=generate_subject_stats)
)


# %% [markdown]
# ## Convert subject ETD to quantity

# %%
# parameters

subject_etd_quantity_params = dict()

# %%
# call the task


subject_etd_quantity = (
    to_quantity.set_task_instance_id("subject_etd_quantity")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(unit="km", **subject_etd_quantity_params)
    .mapvalues(argnames=["value"], argvalues=compute_subject_etd)
)


# %% [markdown]
# ## Create single value widgets for subject ETD per group

# %%
# parameters

total_etd_sv_widgets_params = dict()

# %%
# call the task


total_etd_sv_widgets = (
    create_single_value_widget_single_view.set_task_instance_id("total_etd_sv_widgets")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            never,
        ],
        unpack_depth=1,
    )
    .partial(title="ETD", decimal_places=2, **total_etd_sv_widgets_params)
    .map(argnames=["view", "data"], argvalues=subject_etd_quantity)
)


# %% [markdown]
# ## Merge per group total subject ETD SV widgets

# %%
# parameters

subject_etd_grouped_sv_widget_params = dict()

# %%
# call the task


subject_etd_grouped_sv_widget = (
    merge_widget_views.set_task_instance_id("subject_etd_grouped_sv_widget")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(widgets=total_etd_sv_widgets, **subject_etd_grouped_sv_widget_params)
    .call()
)


# %% [markdown]
# ## Calculate subject distance travelled

# %%
# parameters

compute_sdt_params = dict()

# %%
# call the task


compute_sdt = (
    dataframe_column_sum.set_task_instance_id("compute_sdt")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(column_name="distance_travelled", **compute_sdt_params)
    .mapvalues(argnames=["df"], argvalues=generate_subject_stats)
)


# %% [markdown]
# ## Convert subject distance travelled to quantity

# %%
# parameters

subject_dtq_params = dict()

# %%
# call the task


subject_dtq = (
    to_quantity.set_task_instance_id("subject_dtq")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(unit="km", **subject_dtq_params)
    .mapvalues(argnames=["value"], argvalues=compute_sdt)
)


# %% [markdown]
# ## Create single value widgets for subject distance travelled per group

# %%
# parameters

tdt_sv_widgets_params = dict()

# %%
# call the task


tdt_sv_widgets = (
    create_single_value_widget_single_view.set_task_instance_id("tdt_sv_widgets")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            never,
        ],
        unpack_depth=1,
    )
    .partial(title="Distance Travelled", decimal_places=2, **tdt_sv_widgets_params)
    .map(argnames=["view", "data"], argvalues=subject_dtq)
)


# %% [markdown]
# ## Merge per group total subject distance travelled SV widgets

# %%
# parameters

sdtg_sv_widget_params = dict()

# %%
# call the task


sdtg_sv_widget = (
    merge_widget_views.set_task_instance_id("sdtg_sv_widget")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(widgets=tdt_sv_widgets, **sdtg_sv_widget_params)
    .call()
)


# %% [markdown]
# ## Calculate subject max displacement

# %%
# parameters

compute_subject_max_displacement_params = dict()

# %%
# call the task


compute_subject_max_displacement = (
    dataframe_column_sum.set_task_instance_id("compute_subject_max_displacement")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(column_name="max_displacement", **compute_subject_max_displacement_params)
    .mapvalues(argnames=["df"], argvalues=generate_subject_stats)
)


# %% [markdown]
# ## Convert subject max displacement to quantity

# %%
# parameters

smd_quantity_params = dict()

# %%
# call the task


smd_quantity = (
    to_quantity.set_task_instance_id("smd_quantity")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(unit="km", **smd_quantity_params)
    .mapvalues(argnames=["value"], argvalues=compute_subject_max_displacement)
)


# %% [markdown]
# ## Create single value widgets for subject max displacement per group

# %%
# parameters

tmd_sv_widgets_params = dict()

# %%
# call the task


tmd_sv_widgets = (
    create_single_value_widget_single_view.set_task_instance_id("tmd_sv_widgets")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            never,
        ],
        unpack_depth=1,
    )
    .partial(title="Max Displacement", decimal_places=2, **tmd_sv_widgets_params)
    .map(argnames=["view", "data"], argvalues=smd_quantity)
)


# %% [markdown]
# ## Merge per group total subject max displacement SV widgets

# %%
# parameters

smdg_sv_widget_params = dict()

# %%
# call the task


smdg_sv_widget = (
    merge_widget_views.set_task_instance_id("smdg_sv_widget")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(widgets=tmd_sv_widgets, **smdg_sv_widget_params)
    .call()
)


# %% [markdown]
# ## Calculate subject night day ratio

# %%
# parameters

compute_subject_night_day_ratio_params = dict()

# %%
# call the task


compute_subject_night_day_ratio = (
    dataframe_column_sum.set_task_instance_id("compute_subject_night_day_ratio")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(column_name="night_day_ratio", **compute_subject_night_day_ratio_params)
    .mapvalues(argnames=["df"], argvalues=generate_subject_stats)
)


# %% [markdown]
# ## Create single value widgets for subject night day ratio per group

# %%
# parameters

total_night_day_ratio_sv_widgets_params = dict()

# %%
# call the task


total_night_day_ratio_sv_widgets = (
    create_single_value_widget_single_view.set_task_instance_id(
        "total_night_day_ratio_sv_widgets"
    )
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            never,
        ],
        unpack_depth=1,
    )
    .partial(
        title="Night Day Ratio",
        decimal_places=2,
        **total_night_day_ratio_sv_widgets_params,
    )
    .map(argnames=["view", "data"], argvalues=compute_subject_night_day_ratio)
)


# %% [markdown]
# ## Merge per group total subject night day ratio SV widgets

# %%
# parameters

sndrs_sv_widget_params = dict()

# %%
# call the task


sndrs_sv_widget = (
    merge_widget_views.set_task_instance_id("sndrs_sv_widget")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(widgets=total_night_day_ratio_sv_widgets, **sndrs_sv_widget_params)
    .call()
)


# %% [markdown]
# ## Combine homerange path with zoom value

# %%
# parameters

zip_hr_value_params = dict()

# %%
# call the task


zip_hr_value = (
    zip_groupbykey.set_task_instance_id("zip_hr_value")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        sequences=[zoom_speed_gdf_extent, persist_homerange_html], **zip_hr_value_params
    )
    .call()
)


# %% [markdown]
# ## Convert homerange map html to png

# %%
# parameters

convert_homerange_png_params = dict()

# %%
# call the task


convert_homerange_png = (
    zoom_map_and_screenshot.set_task_instance_id("convert_homerange_png")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        screenshot_config={
            "full_page": False,
            "device_scale_factor": 2.0,
            "wait_for_timeout": 40000,
            "max_concurrent_pages": 1,
            "width": 602,
            "height": 855,
        },
        **convert_homerange_png_params,
    )
    .mapvalues(argnames=["view_state", "input_file"], argvalues=zip_hr_value)
)


# %% [markdown]
# ## Combine speedmap path with zoom value

# %%
# parameters

zip_speed_value_params = dict()

# %%
# call the task


zip_speed_value = (
    zip_groupbykey.set_task_instance_id("zip_speed_value")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        sequences=[zoom_speed_gdf_extent, persist_speedmap_html],
        **zip_speed_value_params,
    )
    .call()
)


# %% [markdown]
# ## Convert speedmap html to png

# %%
# parameters

convert_speedmap_png_params = dict()

# %%
# call the task


convert_speedmap_png = (
    zoom_map_and_screenshot.set_task_instance_id("convert_speedmap_png")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        screenshot_config={
            "full_page": False,
            "device_scale_factor": 2.0,
            "wait_for_timeout": 40000,
            "max_concurrent_pages": 1,
            "width": 1280,
            "height": 720,
        },
        **convert_speedmap_png_params,
    )
    .mapvalues(argnames=["view_state", "input_file"], argvalues=zip_speed_value)
)


# %% [markdown]
# ## Combine seasonal map path with zoom value

# %%
# parameters

zip_seasonal_value_params = dict()

# %%
# call the task


zip_seasonal_value = (
    zip_groupbykey.set_task_instance_id("zip_seasonal_value")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        sequences=[zoom_speed_gdf_extent, persist_seasonal_home_range_html],
        **zip_seasonal_value_params,
    )
    .call()
)


# %% [markdown]
# ## Convert seasonal homerange html to png

# %%
# parameters

convert_season_png_params = dict()

# %%
# call the task


convert_season_png = (
    zoom_map_and_screenshot.set_task_instance_id("convert_season_png")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        screenshot_config={
            "full_page": False,
            "device_scale_factor": 2.0,
            "wait_for_timeout": 40000,
            "max_concurrent_pages": 1,
            "width": 602,
            "height": 855,
        },
        **convert_season_png_params,
    )
    .mapvalues(argnames=["view_state", "input_file"], argvalues=zip_seasonal_value)
)


# %% [markdown]
# ## Convert nsd plot html to png

# %%
# parameters

convert_nsd_png_params = dict()

# %%
# call the task


convert_nsd_png = (
    html_to_png.set_task_instance_id("convert_nsd_png")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        config={
            "full_page": False,
            "device_scale_factor": 2.0,
            "wait_for_timeout": 5,
            "max_concurrent_pages": 3,
            "width": 2238,
            "height": 450,
        },
        **convert_nsd_png_params,
    )
    .mapvalues(argnames=["html_path"], argvalues=persist_nsd_html_urls)
)


# %% [markdown]
# ## Convert mcp plot html to png

# %%
# parameters

convert_mcp_png_params = dict()

# %%
# call the task


convert_mcp_png = (
    html_to_png.set_task_instance_id("convert_mcp_png")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        config={
            "full_page": False,
            "device_scale_factor": 2.0,
            "wait_for_timeout": 5,
            "max_concurrent_pages": 1,
            "width": 2238,
            "height": 450,
        },
        **convert_mcp_png_params,
    )
    .mapvalues(argnames=["html_path"], argvalues=persist_mcp_html_urls)
)


# %% [markdown]
# ## Convert speed plot html to png

# %%
# parameters

convert_speed_png_params = dict()

# %%
# call the task


convert_speed_png = (
    html_to_png.set_task_instance_id("convert_speed_png")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        config={
            "full_page": False,
            "device_scale_factor": 2.0,
            "wait_for_timeout": 5,
            "max_concurrent_pages": 1,
            "width": 2238,
            "height": 450,
        },
        **convert_speed_png_params,
    )
    .mapvalues(argnames=["html_path"], argvalues=persist_speed_html_urls)
)


# %% [markdown]
# ## Convert collared events html to png

# %%
# parameters

convert_events_png_params = dict()

# %%
# call the task


convert_events_png = (
    html_to_png.set_task_instance_id("convert_events_png")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        config={
            "full_page": False,
            "device_scale_factor": 2.0,
            "wait_for_timeout": 5,
            "max_concurrent_pages": 1,
            "width": 2238,
            "height": 450,
        },
        **convert_events_png_params,
    )
    .mapvalues(argnames=["html_path"], argvalues=persist_collared_subject_plots)
)


# %% [markdown]
# ## Unique subjects on trajectories

# %%
# parameters

unique_subjects_params = dict()

# %%
# call the task


unique_subjects = (
    dataframe_column_nunique.set_task_instance_id("unique_subjects")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(df=rename_traj_cols, column_name="subject_name", **unique_subjects_params)
    .call()
)


# %% [markdown]
# ## Download mapbook cover page templates

# %%
# parameters

download_cover_page_params = dict()

# %%
# call the task


download_cover_page = (
    fetch_and_persist_file.set_task_instance_id("download_cover_page")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        url="https://www.dropbox.com/scl/fi/nfv96xs38r3wunp6y866f/cer_cover_page.docx?rlkey=sbl545v87g94tolfafwyfd8b8&st=oyydvpy9&dl=0",
        output_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        overwrite_existing=False,
        unzip=False,
        retries=2,
        **download_cover_page_params,
    )
    .call()
)


# %% [markdown]
# ## Download mep subject templates

# %%
# parameters

download_sect_templates_params = dict()

# %%
# call the task


download_sect_templates = (
    fetch_and_persist_file.set_task_instance_id("download_sect_templates")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        url="https://www.dropbox.com/scl/fi/4symf1385ksnh8mu8sx9v/mep_subject_template_two.docx?rlkey=v5f26c3aiadaasnilhc76owgr&st=wz6mce8l&dl=0",
        output_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        overwrite_existing=False,
        unzip=False,
        retries=2,
        **download_sect_templates_params,
    )
    .call()
)


# %% [markdown]
# ## Create cover template context

# %%
# parameters

create_cover_tpl_context_params = dict()

# %%
# call the task


create_cover_tpl_context = (
    create_mep_ctx_cover.set_task_instance_id("create_cover_tpl_context")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        count=unique_subjects,
        report_period=time_range,
        prepared_by="Ecoscope",
        **create_cover_tpl_context_params,
    )
    .call()
)


# %% [markdown]
# ## Persist cover page context

# %%
# parameters

persist_cover_context_params = dict()

# %%
# call the task


persist_cover_context = (
    create__mep_context_page.set_task_instance_id("persist_cover_context")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        template_path=download_cover_page,
        output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        context=create_cover_tpl_context,
        filename="mep_context.docx",
        **persist_cover_context_params,
    )
    .call()
)


# %% [markdown]
# ## Create subject report context

# %%
# parameters

group_subject_report_context_params = dict()

# %%
# call the task


group_subject_report_context = (
    zip_groupbykey.set_task_instance_id("group_subject_report_context")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        sequences=[
            download_profile_pic,
            persist_subject_info,
            convert_speedmap_png,
            convert_homerange_png,
            convert_season_png,
            convert_nsd_png,
            convert_speed_png,
            convert_events_png,
            convert_mcp_png,
            persist_subject_stats,
            persist_subject_occupancy,
        ],
        **group_subject_report_context_params,
    )
    .call()
)


# %% [markdown]
# ## Create mep subject report context

# %%
# parameters

create_subject_context_params = dict()

# %%
# call the task


create_subject_context = (
    create_mep_subject_context.set_task_instance_id("create_subject_context")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            never,
        ],
        unpack_depth=1,
    )
    .partial(**create_subject_context_params)
    .mapvalues(
        argnames=[
            "profile_photo_path",
            "subject_info_path",
            "speedmap_path",
            "homerange_map_path",
            "seasonal_homerange_map_path",
            "nsd_plot_path",
            "speed_plot_path",
            "collared_event_plot_path",
            "mcp_plot_path",
            "subject_stats_table_path",
            "subject_occupancy_table_path",
        ],
        argvalues=group_subject_report_context,
    )
)


# %% [markdown]
# ## Persist subject report context

# %%
# parameters

persist_subject_report_context_params = dict()

# %%
# call the task


persist_subject_report_context = (
    create_mep_grouper_page.set_task_instance_id("persist_subject_report_context")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        template_path=download_sect_templates,
        output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        filename=None,
        validate_images=True,
        **persist_subject_report_context_params,
    )
    .mapvalues(argnames=["context"], argvalues=create_subject_context)
)


# %% [markdown]
# ## Merge mep word docs

# %%
# parameters

merge_mep_docx_params = dict()

# %%
# call the task


merge_mep_docx = (
    merge_mapbook_files.set_task_instance_id("merge_mep_docx")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        cover_page_path=persist_cover_context,
        output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        context_page_items=persist_subject_report_context,
        filename=None,
        **merge_mep_docx_params,
    )
    .call()
)


# %% [markdown]
# ## Collared Elephant Report Dashboard

# %%
# parameters

collared_report_template_params = dict(
    warning=...,
)

# %%
# call the task


collared_report_template = (
    gather_dashboard.set_task_instance_id("collared_report_template")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        details=workflow_details,
        widgets=[
            national_pa_grouped_sv_widget,
            community_pa_grouped_sv_widget,
            crop_raid_sv_widget,
            kenya_use_grouped_sv_widget,
            unprotected_grouped_sv_widget,
            subject_mcp_grouped_sv_widget,
            subject_etd_grouped_sv_widget,
            sdtg_sv_widget,
            smdg_sv_widget,
            sndrs_sv_widget,
            merge_speedmap_widgets,
            merge_homerange_widgets,
            merge_seasonal_hr_widgets,
            grouped_nsd_plot_widget,
            grouped_speed_plot_widget,
            grouped_collared_widget,
            grouped_mcp_plot_widget,
        ],
        time_range=time_range,
        groupers=groupers,
        **collared_report_template_params,
    )
    .call()
)
