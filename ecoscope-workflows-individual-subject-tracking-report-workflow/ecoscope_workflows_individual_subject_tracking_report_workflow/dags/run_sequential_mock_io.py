# AUTOGENERATED BY ECOSCOPE-WORKFLOWS; see fingerprint in README.md for details

# ruff: noqa: E402

"""WARNING: This file is generated in a testing context and should not be used in production.
Lines specific to the testing context are marked with a test tube emoji (ðŸ§ª) to indicate
that they would not be included (or would be different) in the production version of this file.
"""

import json
import os
import warnings  # ðŸ§ª

from ecoscope_workflows_core.tasks.config import set_string_var as set_string_var
from ecoscope_workflows_core.tasks.config import (
    set_workflow_details as set_workflow_details,
)
from ecoscope_workflows_core.tasks.filter import set_time_range as set_time_range
from ecoscope_workflows_core.tasks.groupby import set_groupers as set_groupers
from ecoscope_workflows_core.tasks.io import set_er_connection as set_er_connection
from ecoscope_workflows_core.tasks.io import set_gee_connection as set_gee_connection
from ecoscope_workflows_core.tasks.skip import (
    any_dependency_skipped as any_dependency_skipped,
)
from ecoscope_workflows_core.tasks.skip import any_is_empty_df as any_is_empty_df
from ecoscope_workflows_core.testing import create_task_magicmock  # ðŸ§ª
from ecoscope_workflows_ext_custom.tasks.io import load_df as load_df
from ecoscope_workflows_ext_custom.tasks.results import (
    set_base_maps_pydeck as set_base_maps_pydeck,
)
from ecoscope_workflows_ext_custom.tasks.transformation import (
    filter_row_values as filter_row_values,
)
from ecoscope_workflows_ext_ecoscope.tasks.transformation import (
    normalize_json_column as normalize_json_column,
)
from ecoscope_workflows_ext_mep.tasks import get_subject_df as get_subject_df
from ecoscope_workflows_ext_mnc.tasks import transform_columns as transform_columns
from ecoscope_workflows_ext_ste.tasks import (
    annotate_gdf_dict_with_geom_type as annotate_gdf_dict_with_geom_type,
)
from ecoscope_workflows_ext_ste.tasks import (
    create_custom_text_layer as create_custom_text_layer,
)
from ecoscope_workflows_ext_ste.tasks import (
    create_deckgl_layers_from_gdf_dict as create_deckgl_layers_from_gdf_dict,
)
from ecoscope_workflows_ext_ste.tasks import filter_df_cols as filter_df_cols
from ecoscope_workflows_ext_ste.tasks import get_file_path as get_file_path
from ecoscope_workflows_ext_ste.tasks import split_gdf_by_column as split_gdf_by_column

get_subjectgroup_observations = create_task_magicmock(  # ðŸ§ª
    anchor="ecoscope_workflows_ext_ecoscope.tasks.io",  # ðŸ§ª
    func_name="get_subjectgroup_observations",  # ðŸ§ª
)  # ðŸ§ª
from ecoscope_workflows_core.tasks.groupby import split_groups as split_groups
from ecoscope_workflows_core.tasks.transformation import map_columns as map_columns
from ecoscope_workflows_ext_ecoscope.tasks.io import persist_df as persist_df
from ecoscope_workflows_ext_ecoscope.tasks.preprocessing import (
    process_relocations as process_relocations,
)
from ecoscope_workflows_ext_mep.tasks import compute_maturity as compute_maturity
from ecoscope_workflows_ext_mep.tasks import (
    persist_subject_photo as persist_subject_photo,
)
from ecoscope_workflows_ext_mep.tasks import (
    process_subject_information as process_subject_information,
)

get_events = create_task_magicmock(  # ðŸ§ª
    anchor="ecoscope_workflows_ext_ecoscope.tasks.io",  # ðŸ§ª
    func_name="get_events",  # ðŸ§ª
)  # ðŸ§ª
from ecoscope_workflows_core.tasks.io import persist_text as persist_text
from ecoscope_workflows_core.tasks.results import (
    create_map_widget_single_view as create_map_widget_single_view,
)
from ecoscope_workflows_core.tasks.results import (
    merge_widget_views as merge_widget_views,
)
from ecoscope_workflows_core.tasks.transformation import (
    add_temporal_index as add_temporal_index,
)
from ecoscope_workflows_core.tasks.transformation import sort_values as sort_values
from ecoscope_workflows_ext_custom.tasks.results import (
    create_path_layer as create_path_layer,
)
from ecoscope_workflows_ext_custom.tasks.results import draw_map as draw_map
from ecoscope_workflows_ext_ecoscope.tasks.analysis import (
    calculate_elliptical_time_density as calculate_elliptical_time_density,
)
from ecoscope_workflows_ext_ecoscope.tasks.preprocessing import (
    relocations_to_trajectory as relocations_to_trajectory,
)
from ecoscope_workflows_ext_ecoscope.tasks.transformation import (
    apply_classification as apply_classification,
)
from ecoscope_workflows_ext_ecoscope.tasks.transformation import (
    apply_color_map as apply_color_map,
)
from ecoscope_workflows_ext_mep.tasks import (
    custom_view_state_deck_gdf as custom_view_state_deck_gdf,
)
from ecoscope_workflows_ext_ste.tasks import (
    combine_deckgl_map_layers as combine_deckgl_map_layers,
)
from ecoscope_workflows_ext_ste.tasks import (
    custom_trajectory_segment_filter as custom_trajectory_segment_filter,
)
from ecoscope_workflows_ext_ste.tasks import envelope_gdf as envelope_gdf
from ecoscope_workflows_ext_ste.tasks import view_state_deck_gdf as view_state_deck_gdf
from ecoscope_workflows_ext_ste.tasks import zip_groupbykey as zip_groupbykey

determine_season_windows = create_task_magicmock(  # ðŸ§ª
    anchor="ecoscope_workflows_ext_ecoscope.tasks.io",  # ðŸ§ª
    func_name="determine_season_windows",  # ðŸ§ª
)  # ðŸ§ª
from ecoscope_workflows_core.tasks.analysis import (
    dataframe_column_nunique as dataframe_column_nunique,
)
from ecoscope_workflows_core.tasks.analysis import (
    dataframe_column_sum as dataframe_column_sum,
)
from ecoscope_workflows_core.tasks.results import (
    create_single_value_widget_single_view as create_single_value_widget_single_view,
)
from ecoscope_workflows_core.tasks.results import gather_dashboard as gather_dashboard
from ecoscope_workflows_core.tasks.skip import never as never
from ecoscope_workflows_ext_custom.tasks.io import html_to_png as html_to_png
from ecoscope_workflows_ext_custom.tasks.results import (
    create_geojson_layer as create_geojson_layer,
)
from ecoscope_workflows_ext_custom.tasks.transformation import (
    to_quantity as to_quantity,
)
from ecoscope_workflows_ext_mep.tasks import (
    build_template_region_lookup as build_template_region_lookup,
)
from ecoscope_workflows_ext_mep.tasks import (
    compute_subject_occupancy as compute_subject_occupancy,
)
from ecoscope_workflows_ext_mep.tasks import (
    compute_subject_stats as compute_subject_stats,
)
from ecoscope_workflows_ext_mep.tasks import (
    compute_template_regions as compute_template_regions,
)
from ecoscope_workflows_ext_mep.tasks import (
    create__mep_context_page as create__mep_context_page,
)
from ecoscope_workflows_ext_mep.tasks import (
    create_mep_ctx_cover as create_mep_ctx_cover,
)
from ecoscope_workflows_ext_mep.tasks import (
    create_mep_grouper_page as create_mep_grouper_page,
)
from ecoscope_workflows_ext_mep.tasks import (
    create_mep_subject_context as create_mep_subject_context,
)
from ecoscope_workflows_ext_mep.tasks import (
    draw_season_collared_plot as draw_season_collared_plot,
)
from ecoscope_workflows_ext_mep.tasks import (
    draw_season_mcp_plot as draw_season_mcp_plot,
)
from ecoscope_workflows_ext_mep.tasks import (
    draw_season_nsd_plot as draw_season_nsd_plot,
)
from ecoscope_workflows_ext_mep.tasks import (
    draw_season_speed_plot as draw_season_speed_plot,
)
from ecoscope_workflows_ext_mep.tasks import (
    zoom_map_and_screenshot as zoom_map_and_screenshot,
)
from ecoscope_workflows_ext_ste.tasks import (
    calculate_seasonal_home_range as calculate_seasonal_home_range,
)
from ecoscope_workflows_ext_ste.tasks import convert_to_str as convert_to_str
from ecoscope_workflows_ext_ste.tasks import (
    create_seasonal_labels as create_seasonal_labels,
)
from ecoscope_workflows_ext_ste.tasks import (
    fetch_and_persist_file as fetch_and_persist_file,
)
from ecoscope_workflows_ext_ste.tasks import generate_mcp_gdf as generate_mcp_gdf
from ecoscope_workflows_ext_ste.tasks import merge_mapbook_files as merge_mapbook_files

from ..params import Params


def main(params: Params):
    warnings.warn("This test script should not be used in production!")  # ðŸ§ª

    params_dict = json.loads(params.model_dump_json(exclude_unset=True))

    workflow_details = (
        set_workflow_details.validate()
        .set_task_instance_id("workflow_details")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(**(params_dict.get("workflow_details") or {}))
        .call()
    )

    time_range = (
        set_time_range.validate()
        .set_task_instance_id("time_range")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(**(params_dict.get("time_range") or {}))
        .call()
    )

    groupers = (
        set_groupers.validate()
        .set_task_instance_id("groupers")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            groupers=[{"index_name": "subject_name"}],
            **(params_dict.get("groupers") or {}),
        )
        .call()
    )

    configure_base_maps = (
        set_base_maps_pydeck.validate()
        .set_task_instance_id("configure_base_maps")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(**(params_dict.get("configure_base_maps") or {}))
        .call()
    )

    er_client_name = (
        set_er_connection.validate()
        .set_task_instance_id("er_client_name")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(**(params_dict.get("er_client_name") or {}))
        .call()
    )

    gee_project_name = (
        set_gee_connection.validate()
        .set_task_instance_id("gee_project_name")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(**(params_dict.get("gee_project_name") or {}))
        .call()
    )

    subject_group_var = (
        set_string_var.validate()
        .set_task_instance_id("subject_group_var")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(**(params_dict.get("subject_group_var") or {}))
        .call()
    )

    retrieve_ldx_db = (
        get_file_path.validate()
        .set_task_instance_id("retrieve_ldx_db")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            output_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            **(params_dict.get("retrieve_ldx_db") or {}),
        )
        .call()
    )

    load_ldx = (
        load_df.validate()
        .set_task_instance_id("load_ldx")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            file_path=retrieve_ldx_db,
            layer="landDx_polygons",
            deserialize_json=False,
            **(params_dict.get("load_ldx") or {}),
        )
        .call()
    )

    filter_ldx_aoi = (
        filter_row_values.validate()
        .set_task_instance_id("filter_ldx_aoi")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=load_ldx,
            column="type",
            values=["Community Conservancy", "National Reserve", "National Park"],
            **(params_dict.get("filter_ldx_aoi") or {}),
        )
        .call()
    )

    filter_ldx_cols = (
        filter_df_cols.validate()
        .set_task_instance_id("filter_ldx_cols")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=filter_ldx_aoi,
            columns=["type", "name", "geometry"],
            **(params_dict.get("filter_ldx_cols") or {}),
        )
        .call()
    )

    create_ldx_text_layer = (
        create_custom_text_layer.validate()
        .set_task_instance_id("create_ldx_text_layer")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            geodataframe=filter_ldx_cols,
            layer_style={
                "get_text": "name",
                "get_color": [0, 0, 0, 255],
                "get_size": 1000,
                "size_units": "meters",
                "size_min_pixels": 40,
                "size_max_pixels": 75,
                "size_scale": 1.25,
                "font_family": "Arial",
                "font_weight": "normal",
                "get_text_anchor": "middle",
                "get_alignment_baseline": "center",
                "billboard": True,
                "background_padding": [4, 8],
                "pickable": True,
                "auto_highlight": False,
            },
            use_centroid=True,
            legend=None,
            **(params_dict.get("create_ldx_text_layer") or {}),
        )
        .call()
    )

    split_ldx_by_type = (
        split_gdf_by_column.validate()
        .set_task_instance_id("split_ldx_by_type")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            gdf=filter_ldx_cols,
            column="type",
            **(params_dict.get("split_ldx_by_type") or {}),
        )
        .call()
    )

    annotate_gdf_dict = (
        annotate_gdf_dict_with_geom_type.validate()
        .set_task_instance_id("annotate_gdf_dict")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            gdf_dict=split_ldx_by_type, **(params_dict.get("annotate_gdf_dict") or {})
        )
        .call()
    )

    create_ldx_styled_layers = (
        create_deckgl_layers_from_gdf_dict.validate()
        .set_task_instance_id("create_ldx_styled_layers")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            gdf_dict=annotate_gdf_dict,
            styles={
                "Community Conservancy": {
                    "get_fill_color": [166, 182, 151],
                    "get_line_color": [166, 182, 151],
                    "opacity": 0.15,
                    "stroked": True,
                    "get_line_width": 2.0,
                },
                "National Reserve": {
                    "get_fill_color": [136, 167, 142],
                    "get_line_color": [136, 167, 142],
                    "opacity": 0.15,
                    "stroked": True,
                    "get_line_width": 2.0,
                },
                "National Park": {
                    "get_fill_color": [17, 86, 49],
                    "get_line_color": [17, 86, 49],
                    "opacity": 0.15,
                    "stroked": True,
                    "get_line_width": 2.0,
                },
            },
            legends={
                "title": "Land Use",
                "values": [
                    {"label": "Community Conservancy", "color": "#a6b697"},
                    {"label": "National Reserve", "color": "#88a78e"},
                    {"label": "National Park", "color": "#115631"},
                ],
            },
            **(params_dict.get("create_ldx_styled_layers") or {}),
        )
        .call()
    )

    retrieve_subjects_df = (
        get_subject_df.validate()
        .set_task_instance_id("retrieve_subjects_df")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            client=er_client_name,
            include_inactive=True,
            bbox=None,
            subject_group_id=None,
            subject_group_name=subject_group_var,
            name=None,
            updated_since=None,
            updated_until=None,
            tracks=None,
            ids=None,
            max_ids_per_request=50,
            raise_on_empty=True,
            **(params_dict.get("retrieve_subjects_df") or {}),
        )
        .call()
    )

    normalize_subject_info = (
        normalize_json_column.validate()
        .set_task_instance_id("normalize_subject_info")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            column="additional",
            df=retrieve_subjects_df,
            skip_if_not_exists=True,
            sort_columns=True,
            **(params_dict.get("normalize_subject_info") or {}),
        )
        .call()
    )

    rename_subject_cols = (
        transform_columns.validate()
        .set_task_instance_id("rename_subject_cols")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            drop_columns=[
                "url",
                "image_url",
                "common_name",
                "content_type",
                "additional__external_id",
                "additional__tm_animal_id",
                "additional__external_name",
            ],
            retain_columns=[],
            rename_columns={
                "id": "groupby_col",
                "name": "subject_name",
                "hex": "hex_color",
                "additional__rgb": "rgb",
                "additional__Bio": "subject_bio",
                "additional__sex": "subject_sex",
                "additional__DOB": "date_of_birth",
                "additional__notes": "notes",
                "additional__status": "status",
                "additional__region": "region",
                "additional__country": "country",
                "additional__id_photo": "photo",
                "additional__distribution": "distribution",
            },
            skip_missing_rename=True,
            required_columns=["id", "name"],
            df=normalize_subject_info,
            **(params_dict.get("rename_subject_cols") or {}),
        )
        .call()
    )

    subject_observations = (
        get_subjectgroup_observations.validate()
        .set_task_instance_id("subject_observations")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            filter="clean",
            client=er_client_name,
            time_range=time_range,
            subject_group_name=subject_group_var,
            raise_on_empty=False,
            include_details=False,
            include_subjectsource_details=False,
            **(params_dict.get("subject_observations") or {}),
        )
        .call()
    )

    subject_reloc = (
        process_relocations.validate()
        .set_task_instance_id("subject_reloc")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            observations=subject_observations,
            relocs_columns=[
                "groupby_col",
                "fixtime",
                "junk_status",
                "geometry",
                "extra__subject__name",
                "extra__subject__hex",
                "extra__subject__sex",
                "extra__created_at",
                "extra__subject__subject_subtype",
            ],
            filter_point_coords=[
                {"x": 180.0, "y": 90.0},
                {"x": 0.0, "y": 0.0},
                {"x": 1.0, "y": 1.0},
            ],
            **(params_dict.get("subject_reloc") or {}),
        )
        .call()
    )

    rename_reloc_cols = (
        map_columns.validate()
        .set_task_instance_id("rename_reloc_cols")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            raise_if_not_found=True,
            drop_columns=[],
            retain_columns=[],
            rename_columns={
                "name": "subject_name",
                "hex": "hex_color",
                "sex": "subject_sex",
                "subject_subtype": "subject_subtype",
                "created_at": "created_at",
            },
            df=subject_reloc,
            **(params_dict.get("rename_reloc_cols") or {}),
        )
        .call()
    )

    compute_subject_maturity = (
        compute_maturity.validate()
        .set_task_instance_id("compute_subject_maturity")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            subject_df=rename_subject_cols,
            relocations_gdf=rename_reloc_cols,
            months_duration=6,
            time_column="fixtime",
            **(params_dict.get("compute_subject_maturity") or {}),
        )
        .call()
    )

    split_subject_by_group = (
        split_groups.validate()
        .set_task_instance_id("split_subject_by_group")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=compute_subject_maturity,
            groupers=groupers,
            **(params_dict.get("split_subject_by_group") or {}),
        )
        .call()
    )

    download_profile_pic = (
        persist_subject_photo.validate()
        .set_task_instance_id("download_profile_pic")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            image_type=".png",
            column="photo",
            overwrite_existing=True,
            output_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            **(params_dict.get("download_profile_pic") or {}),
        )
        .mapvalues(argnames=["subject_df"], argvalues=split_subject_by_group)
    )

    download_subject_info = (
        process_subject_information.validate()
        .set_task_instance_id("download_subject_info")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            maxlen=1000,
            output_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            **(params_dict.get("download_subject_info") or {}),
        )
        .mapvalues(argnames=["subject_df"], argvalues=split_subject_by_group)
    )

    persist_subject_info = (
        persist_df.validate()
        .set_task_instance_id("persist_subject_info")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filetype="csv",
            filename=None,
            **(params_dict.get("persist_subject_info") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=download_subject_info)
    )

    get_events_data = (
        get_events.validate()
        .set_task_instance_id("get_events_data")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            client=er_client_name,
            time_range=time_range,
            include_details=True,
            raise_on_empty=False,
            include_null_geometry=True,
            include_updates=False,
            include_related_events=False,
            include_display_values=False,
            event_types=["mep_collar_check", "mep_collaring", "mep_source_failure"],
            event_columns=[
                "id",
                "time",
                "event_type",
                "event_category",
                "reported_by",
                "serial_number",
                "geometry",
                "event_details",
                "priority",
                "priority_label",
            ],
            **(params_dict.get("get_events_data") or {}),
        )
        .call()
    )

    normalize_event_details = (
        normalize_json_column.validate()
        .set_task_instance_id("normalize_event_details")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            column="event_details",
            df=get_events_data,
            skip_if_not_exists=True,
            sort_columns=True,
            **(params_dict.get("normalize_event_details") or {}),
        )
        .call()
    )

    rename_event_cols = (
        transform_columns.validate()
        .set_task_instance_id("rename_event_cols")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            drop_columns=[],
            retain_columns=[],
            rename_columns={
                "event_details__pic": "pic",
                "event_details__region": "region",
                "event_details__source": "source",
                "event_details__details": "details",
                "event_details__subject": "groupby_col",
                "event_details__source_id": "source_id",
                "event_details__subject_id": "subject_name",
                "event_details__collaring_type": "collaring_type",
                "event_details__collaring_reason": "collaring_reason",
                "event_details__collar_checked_by": "collar_checked_by",
            },
            skip_missing_rename=True,
            required_columns=["event_details__subject_id"],
            df=normalize_event_details,
            **(params_dict.get("rename_event_cols") or {}),
        )
        .call()
    )

    split_events_by_group = (
        split_groups.validate()
        .set_task_instance_id("split_events_by_group")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=rename_event_cols,
            groupers=groupers,
            **(params_dict.get("split_events_by_group") or {}),
        )
        .call()
    )

    split_relocs_by_group = (
        split_groups.validate()
        .set_task_instance_id("split_relocs_by_group")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=rename_reloc_cols,
            groupers=groupers,
            **(params_dict.get("split_relocs_by_group") or {}),
        )
        .call()
    )

    custom_trajs_filter = (
        custom_trajectory_segment_filter.validate()
        .set_task_instance_id("custom_trajs_filter")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(**(params_dict.get("custom_trajs_filter") or {}))
        .call()
    )

    convert_to_trajectories = (
        relocations_to_trajectory.validate()
        .set_task_instance_id("convert_to_trajectories")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            relocations=rename_reloc_cols,
            trajectory_segment_filter=custom_trajs_filter,
            **(params_dict.get("convert_to_trajectories") or {}),
        )
        .call()
    )

    add_temporal_index_to_traj = (
        add_temporal_index.validate()
        .set_task_instance_id("add_temporal_index_to_traj")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=convert_to_trajectories,
            time_col="segment_start",
            groupers=groupers,
            cast_to_datetime=True,
            format="mixed",
            **(params_dict.get("add_temporal_index_to_traj") or {}),
        )
        .call()
    )

    classify_trajectories_speed_bins = (
        apply_classification.validate()
        .set_task_instance_id("classify_trajectories_speed_bins")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=add_temporal_index_to_traj,
            input_column_name="speed_kmhr",
            output_column_name="speed_bins",
            classification_options={"scheme": "equal_interval", "k": 6},
            label_options={
                "label_ranges": True,
                "label_decimals": 1,
                "label_suffix": " km/h",
            },
            **(params_dict.get("classify_trajectories_speed_bins") or {}),
        )
        .call()
    )

    rename_traj_cols = (
        map_columns.validate()
        .set_task_instance_id("rename_traj_cols")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            raise_if_not_found=True,
            df=classify_trajectories_speed_bins,
            drop_columns=[],
            retain_columns=[],
            rename_columns={
                "extra__hex_color": "hex_color",
                "extra__subject_name": "subject_name",
                "extra__subject_sex": "subject_sex",
                "extra__subject_subtype": "subject_subtype",
                "extra__created_at": "created_at",
            },
            **(params_dict.get("rename_traj_cols") or {}),
        )
        .call()
    )

    split_traj_by_group = (
        split_groups.validate()
        .set_task_instance_id("split_traj_by_group")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=rename_traj_cols,
            groupers=groupers,
            **(params_dict.get("split_traj_by_group") or {}),
        )
        .call()
    )

    sort_trajs_by_speed = (
        sort_values.validate()
        .set_task_instance_id("sort_trajs_by_speed")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            column_name="speed_bins",
            na_position="first",
            ascending=True,
            **(params_dict.get("sort_trajs_by_speed") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=split_traj_by_group)
    )

    apply_speed_colormap = (
        apply_color_map.validate()
        .set_task_instance_id("apply_speed_colormap")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            input_column_name="speed_bins",
            output_column_name="speed_bins_colormap",
            colormap=["#1a9850", "#91cf60", "#d9ef8b", "#fee08b", "#fc8d59", "#d73027"],
            **(params_dict.get("apply_speed_colormap") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=sort_trajs_by_speed)
    )

    filter_speedmap_gdf = (
        filter_df_cols.validate()
        .set_task_instance_id("filter_speedmap_gdf")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            columns=["geometry", "speed_kmhr", "speed_bins", "speed_bins_colormap"],
            **(params_dict.get("filter_speedmap_gdf") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=apply_speed_colormap)
    )

    generate_speedmap_layers = (
        create_path_layer.validate()
        .set_task_instance_id("generate_speedmap_layers")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            layer_style={
                "get_color": "speed_bins_colormap",
                "get_width": 2.55,
                "width_scale": 1,
                "width_min_pixels": 2,
                "width_max_pixels": 8,
                "width_units": "pixels",
                "cap_rounded": True,
                "joint_rounded": True,
                "billboard": False,
                "opacity": 0.45,
                "stroked": True,
            },
            legend={
                "title": "Speed (km/h)",
                "label_column": "speed_bins",
                "color_column": "speed_bins_colormap",
                "sort": "ascending",
                "label_suffix": None,
            },
            **(params_dict.get("generate_speedmap_layers") or {}),
        )
        .mapvalues(argnames=["geodataframe"], argvalues=filter_speedmap_gdf)
    )

    gdf_bounding_extent = (
        envelope_gdf.validate()
        .set_task_instance_id("gdf_bounding_extent")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(**(params_dict.get("gdf_bounding_extent") or {}))
        .mapvalues(argnames=["gdf"], argvalues=filter_speedmap_gdf)
    )

    zoom_gdf_extent = (
        view_state_deck_gdf.validate()
        .set_task_instance_id("zoom_gdf_extent")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(pitch=0, bearing=0, **(params_dict.get("zoom_gdf_extent") or {}))
        .mapvalues(argnames=["gdf"], argvalues=gdf_bounding_extent)
    )

    zoom_speed_gdf_extent = (
        custom_view_state_deck_gdf.validate()
        .set_task_instance_id("zoom_speed_gdf_extent")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            pitch=0,
            bearing=0,
            buffer=0.375,
            map_width_px=900,
            map_height_px=700,
            **(params_dict.get("zoom_speed_gdf_extent") or {}),
        )
        .mapvalues(argnames=["gdf"], argvalues=filter_speedmap_gdf)
    )

    combined_ldx_speed_layers = (
        combine_deckgl_map_layers.validate()
        .set_task_instance_id("combined_ldx_speed_layers")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            static_layers=[create_ldx_styled_layers, create_ldx_text_layer],
            **(params_dict.get("combined_ldx_speed_layers") or {}),
        )
        .mapvalues(argnames=["grouped_layers"], argvalues=generate_speedmap_layers)
    )

    zip_speedmap_with_viewstate = (
        zip_groupbykey.validate()
        .set_task_instance_id("zip_speedmap_with_viewstate")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            sequences=[combined_ldx_speed_layers, zoom_gdf_extent],
            **(params_dict.get("zip_speedmap_with_viewstate") or {}),
        )
        .call()
    )

    draw_speedmap = (
        draw_map.validate()
        .set_task_instance_id("draw_speedmap")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            tile_layers=configure_base_maps,
            static=False,
            title=None,
            max_zoom=10,
            legend_style={"placement": "bottom-right"},
            **(params_dict.get("draw_speedmap") or {}),
        )
        .mapvalues(
            argnames=["geo_layers", "view_state"], argvalues=zip_speedmap_with_viewstate
        )
    )

    persist_speedmap_html = (
        persist_text.validate()
        .set_task_instance_id("persist_speedmap_html")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filename_suffix="speedmap",
            **(params_dict.get("persist_speedmap_html") or {}),
        )
        .mapvalues(argnames=["text"], argvalues=draw_speedmap)
    )

    create_speedmap_widgets = (
        create_map_widget_single_view.validate()
        .set_task_instance_id("create_speedmap_widgets")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            title="Speed Map", **(params_dict.get("create_speedmap_widgets") or {})
        )
        .map(argnames=["view", "data"], argvalues=persist_speedmap_html)
    )

    merge_speedmap_widgets = (
        merge_widget_views.validate()
        .set_task_instance_id("merge_speedmap_widgets")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            widgets=create_speedmap_widgets,
            **(params_dict.get("merge_speedmap_widgets") or {}),
        )
        .call()
    )

    generate_etd = (
        calculate_elliptical_time_density.validate()
        .set_task_instance_id("generate_etd")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            auto_scale_or_custom_cell_size={"auto_scale_or_custom": "Auto-scale"},
            crs="ESRI:53042",
            percentiles=[50.0, 60.0, 70.0, 80.0, 90.0, 95.0, 99.9],
            nodata_value="nan",
            band_count=1,
            max_speed_factor=1.05,
            expansion_factor=1.3,
            **(params_dict.get("generate_etd") or {}),
        )
        .mapvalues(argnames=["trajectory_gdf"], argvalues=split_traj_by_group)
    )

    determine_seasonal_windows = (
        determine_season_windows.validate()
        .set_task_instance_id("determine_seasonal_windows")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            client=gee_project_name,
            time_range=time_range,
            **(params_dict.get("determine_seasonal_windows") or {}),
        )
        .mapvalues(argnames=["roi"], argvalues=generate_etd)
    )

    persist_subject_seasonal_windows = (
        persist_df.validate()
        .set_task_instance_id("persist_subject_seasonal_windows")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filetype="csv",
            filename=None,
            **(params_dict.get("persist_subject_seasonal_windows") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=determine_seasonal_windows)
    )

    zip_etd_with_traj = (
        zip_groupbykey.validate()
        .set_task_instance_id("zip_etd_with_traj")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            sequences=[determine_seasonal_windows, split_traj_by_group],
            **(params_dict.get("zip_etd_with_traj") or {}),
        )
        .call()
    )

    add_season_labels = (
        create_seasonal_labels.validate()
        .set_task_instance_id("add_season_labels")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(**(params_dict.get("add_season_labels") or {}))
        .mapvalues(argnames=["seasons_df", "trajectories"], argvalues=zip_etd_with_traj)
    )

    generate_mcp = (
        generate_mcp_gdf.validate()
        .set_task_instance_id("generate_mcp")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(planar_crs="ESRI:53042", **(params_dict.get("generate_mcp") or {}))
        .mapvalues(argnames=["gdf"], argvalues=split_traj_by_group)
    )

    apply_etd_colormap = (
        apply_color_map.validate()
        .set_task_instance_id("apply_etd_colormap")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            input_column_name="percentile",
            output_column_name="etd_percentile_colors",
            colormap="RdYlGn",
            **(params_dict.get("apply_etd_colormap") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=generate_etd)
    )

    generate_home_range_layers = (
        create_geojson_layer.validate()
        .set_task_instance_id("generate_home_range_layers")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            layer_style={
                "filled": True,
                "stroked": True,
                "extruded": False,
                "wireframe": False,
                "get_fill_color": "etd_percentile_colors",
                "get_line_color": "etd_percentile_colors",
                "opacity": 0.45,
                "get_line_width": 1.55,
                "get_elevation": 0,
                "get_point_radius": 1,
                "line_width_units": "pixels",
                "line_width_scale": 1,
                "line_width_min_pixels": 1,
                "line_width_max_pixels": 5,
            },
            legend={
                "title": "Home Range Percentiles",
                "label_column": "percentile",
                "color_column": "etd_percentile_colors",
                "sort": "ascending",
                "label_suffix": None,
            },
            **(params_dict.get("generate_home_range_layers") or {}),
        )
        .mapvalues(argnames=["geodataframe"], argvalues=apply_etd_colormap)
    )

    combined_ldx_home_range_layers = (
        combine_deckgl_map_layers.validate()
        .set_task_instance_id("combined_ldx_home_range_layers")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            static_layers=[create_ldx_styled_layers, create_ldx_text_layer],
            **(params_dict.get("combined_ldx_home_range_layers") or {}),
        )
        .mapvalues(argnames=["grouped_layers"], argvalues=generate_home_range_layers)
    )

    zoom_hr_gdf_extent = (
        custom_view_state_deck_gdf.validate()
        .set_task_instance_id("zoom_hr_gdf_extent")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            pitch=0,
            bearing=0,
            buffer=0.375,
            map_width_px=602,
            map_height_px=855,
            **(params_dict.get("zoom_hr_gdf_extent") or {}),
        )
        .mapvalues(argnames=["gdf"], argvalues=apply_etd_colormap)
    )

    zip_hr_with_viewstate = (
        zip_groupbykey.validate()
        .set_task_instance_id("zip_hr_with_viewstate")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            sequences=[combined_ldx_home_range_layers, zoom_gdf_extent],
            **(params_dict.get("zip_hr_with_viewstate") or {}),
        )
        .call()
    )

    draw_home_range_map = (
        draw_map.validate()
        .set_task_instance_id("draw_home_range_map")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            tile_layers=configure_base_maps,
            static=False,
            title=None,
            max_zoom=10,
            legend_style={"placement": "bottom-right"},
            **(params_dict.get("draw_home_range_map") or {}),
        )
        .mapvalues(
            argnames=["geo_layers", "view_state"], argvalues=zip_hr_with_viewstate
        )
    )

    persist_homerange_html = (
        persist_text.validate()
        .set_task_instance_id("persist_homerange_html")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filename_suffix="homerange",
            **(params_dict.get("persist_homerange_html") or {}),
        )
        .mapvalues(argnames=["text"], argvalues=draw_home_range_map)
    )

    create_home_range_widgets = (
        create_map_widget_single_view.validate()
        .set_task_instance_id("create_home_range_widgets")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            title="Home Range", **(params_dict.get("create_home_range_widgets") or {})
        )
        .map(argnames=["view", "data"], argvalues=persist_homerange_html)
    )

    merge_homerange_widgets = (
        merge_widget_views.validate()
        .set_task_instance_id("merge_homerange_widgets")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            widgets=create_home_range_widgets,
            **(params_dict.get("merge_homerange_widgets") or {}),
        )
        .call()
    )

    seasonal_home_range = (
        calculate_seasonal_home_range.validate()
        .set_task_instance_id("seasonal_home_range")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            groupby_cols=["season"],
            percentiles=[99.9],
            auto_scale_or_custom_cell_size={"auto_scale_or_custom": "Auto-scale"},
            **(params_dict.get("seasonal_home_range") or {}),
        )
        .mapvalues(argnames=["gdf"], argvalues=add_season_labels)
    )

    convert_season_to_string = (
        convert_to_str.validate()
        .set_task_instance_id("convert_season_to_string")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            columns=["season"], **(params_dict.get("convert_season_to_string") or {})
        )
        .mapvalues(argnames=["df"], argvalues=seasonal_home_range)
    )

    apply_seasonal_colormap = (
        apply_color_map.validate()
        .set_task_instance_id("apply_seasonal_colormap")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            input_column_name="season",
            output_column_name="season_colors",
            colormap=["#00bfff", "#ff7f50"],
            **(params_dict.get("apply_seasonal_colormap") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=convert_season_to_string)
    )

    generate_season_layers = (
        create_geojson_layer.validate()
        .set_task_instance_id("generate_season_layers")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            layer_style={
                "filled": True,
                "stroked": True,
                "extruded": False,
                "wireframe": False,
                "get_fill_color": "season_colors",
                "get_line_color": [0, 0, 0, 255],
                "opacity": 0.15,
                "get_line_width": 0.35,
                "get_elevation": 0,
                "get_point_radius": 1,
                "line_width_units": "pixels",
                "line_width_scale": 1,
                "line_width_min_pixels": 1,
                "line_width_max_pixels": 5,
            },
            legend={
                "title": "Seasonal Home Range",
                "label_column": "season",
                "color_column": "season_colors",
                "sort": "ascending",
                "label_suffix": None,
            },
            **(params_dict.get("generate_season_layers") or {}),
        )
        .mapvalues(argnames=["geodataframe"], argvalues=apply_seasonal_colormap)
    )

    create_mcp_polygon_layer = (
        create_geojson_layer.validate()
        .set_task_instance_id("create_mcp_polygon_layer")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            layer_style={
                "filled": False,
                "stroked": True,
                "extruded": False,
                "wireframe": False,
                "get_fill_color": [255, 20, 147, 50],
                "get_line_color": [255, 20, 147, 200],
                "opacity": 0.35,
                "get_line_width": 1.75,
                "get_elevation": 0,
                "get_point_radius": 1,
                "line_width_units": "pixels",
                "line_width_scale": 1,
                "line_width_min_pixels": 1,
                "line_width_max_pixels": 5,
            },
            legend={"title": "", "values": [{"label": "MCP", "color": "#ff1493"}]},
            **(params_dict.get("create_mcp_polygon_layer") or {}),
        )
        .mapvalues(argnames=["geodataframe"], argvalues=generate_mcp)
    )

    zip_season_mcp_layer = (
        zip_groupbykey.validate()
        .set_task_instance_id("zip_season_mcp_layer")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            sequences=[create_mcp_polygon_layer, generate_season_layers],
            **(params_dict.get("zip_season_mcp_layer") or {}),
        )
        .call()
    )

    combined_ldx_seasonal_hr_layers = (
        combine_deckgl_map_layers.validate()
        .set_task_instance_id("combined_ldx_seasonal_hr_layers")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            static_layers=[create_ldx_styled_layers, create_ldx_text_layer],
            **(params_dict.get("combined_ldx_seasonal_hr_layers") or {}),
        )
        .mapvalues(argnames=["grouped_layers"], argvalues=zip_season_mcp_layer)
    )

    zoom_seasons_gdf_extent = (
        custom_view_state_deck_gdf.validate()
        .set_task_instance_id("zoom_seasons_gdf_extent")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            pitch=0,
            bearing=0,
            buffer=0.375,
            map_width_px=602,
            map_height_px=855,
            **(params_dict.get("zoom_seasons_gdf_extent") or {}),
        )
        .mapvalues(argnames=["gdf"], argvalues=apply_seasonal_colormap)
    )

    zip_seasonal_hr_with_viewstate = (
        zip_groupbykey.validate()
        .set_task_instance_id("zip_seasonal_hr_with_viewstate")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            sequences=[combined_ldx_seasonal_hr_layers, zoom_gdf_extent],
            **(params_dict.get("zip_seasonal_hr_with_viewstate") or {}),
        )
        .call()
    )

    draw_seasonal_home_range_map = (
        draw_map.validate()
        .set_task_instance_id("draw_seasonal_home_range_map")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            tile_layers=configure_base_maps,
            static=False,
            title=None,
            max_zoom=10,
            legend_style={"placement": "bottom-right"},
            **(params_dict.get("draw_seasonal_home_range_map") or {}),
        )
        .mapvalues(
            argnames=["geo_layers", "view_state"],
            argvalues=zip_seasonal_hr_with_viewstate,
        )
    )

    persist_seasonal_home_range_html = (
        persist_text.validate()
        .set_task_instance_id("persist_seasonal_home_range_html")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filename_suffix="seasonal_home_range",
            **(params_dict.get("persist_seasonal_home_range_html") or {}),
        )
        .mapvalues(argnames=["text"], argvalues=draw_seasonal_home_range_map)
    )

    create_seasonal_hr_widgets = (
        create_map_widget_single_view.validate()
        .set_task_instance_id("create_seasonal_hr_widgets")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            title="Seasonal Home Range",
            **(params_dict.get("create_seasonal_hr_widgets") or {}),
        )
        .map(argnames=["view", "data"], argvalues=persist_seasonal_home_range_html)
    )

    merge_seasonal_hr_widgets = (
        merge_widget_views.validate()
        .set_task_instance_id("merge_seasonal_hr_widgets")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            widgets=create_seasonal_hr_widgets,
            **(params_dict.get("merge_seasonal_hr_widgets") or {}),
        )
        .call()
    )

    zip_relocs_with_seasons = (
        zip_groupbykey.validate()
        .set_task_instance_id("zip_relocs_with_seasons")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            sequences=[determine_seasonal_windows, split_relocs_by_group],
            **(params_dict.get("zip_relocs_with_seasons") or {}),
        )
        .call()
    )

    generate_nsd_seasonal_plot = (
        draw_season_nsd_plot.validate()
        .set_task_instance_id("generate_nsd_seasonal_plot")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(**(params_dict.get("generate_nsd_seasonal_plot") or {}))
        .mapvalues(
            argnames=["seasons_df", "relocations_gdf"],
            argvalues=zip_relocs_with_seasons,
        )
    )

    persist_nsd_html_urls = (
        persist_text.validate()
        .set_task_instance_id("persist_nsd_html_urls")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filename_suffix="nsd_seasonal_plot",
            **(params_dict.get("persist_nsd_html_urls") or {}),
        )
        .mapvalues(argnames=["text"], argvalues=generate_nsd_seasonal_plot)
    )

    nsd_plot_widgets_single_view = (
        create_map_widget_single_view.validate()
        .set_task_instance_id("nsd_plot_widgets_single_view")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(
            title="Net Square Displacement (NSD)",
            **(params_dict.get("nsd_plot_widgets_single_view") or {}),
        )
        .map(argnames=["view", "data"], argvalues=persist_nsd_html_urls)
    )

    grouped_nsd_plot_widget = (
        merge_widget_views.validate()
        .set_task_instance_id("grouped_nsd_plot_widget")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            widgets=nsd_plot_widgets_single_view,
            **(params_dict.get("grouped_nsd_plot_widget") or {}),
        )
        .call()
    )

    generate_speed_seasonal_plot = (
        draw_season_speed_plot.validate()
        .set_task_instance_id("generate_speed_seasonal_plot")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(**(params_dict.get("generate_speed_seasonal_plot") or {}))
        .mapvalues(
            argnames=["seasons_df", "relocations_gdf"],
            argvalues=zip_relocs_with_seasons,
        )
    )

    persist_speed_html_urls = (
        persist_text.validate()
        .set_task_instance_id("persist_speed_html_urls")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filename_suffix="speed_seasonal_plot",
            **(params_dict.get("persist_speed_html_urls") or {}),
        )
        .mapvalues(argnames=["text"], argvalues=generate_speed_seasonal_plot)
    )

    speed_plot_widgets_single_view = (
        create_map_widget_single_view.validate()
        .set_task_instance_id("speed_plot_widgets_single_view")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(
            title="Speed", **(params_dict.get("speed_plot_widgets_single_view") or {})
        )
        .map(argnames=["view", "data"], argvalues=persist_speed_html_urls)
    )

    grouped_speed_plot_widget = (
        merge_widget_views.validate()
        .set_task_instance_id("grouped_speed_plot_widget")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            widgets=speed_plot_widgets_single_view,
            **(params_dict.get("grouped_speed_plot_widget") or {}),
        )
        .call()
    )

    generate_collared_subject_plot = (
        draw_season_collared_plot.validate()
        .set_task_instance_id("generate_collared_subject_plot")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            events_gdf=rename_event_cols,
            filter_col="subject_name",
            **(params_dict.get("generate_collared_subject_plot") or {}),
        )
        .mapvalues(
            argnames=["seasons_df", "relocations_gdf"],
            argvalues=zip_relocs_with_seasons,
        )
    )

    persist_collared_subject_plots = (
        persist_text.validate()
        .set_task_instance_id("persist_collared_subject_plots")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filename_suffix="collared_subject_plot",
            **(params_dict.get("persist_collared_subject_plots") or {}),
        )
        .mapvalues(argnames=["text"], argvalues=generate_collared_subject_plot)
    )

    collared_widget_view = (
        create_map_widget_single_view.validate()
        .set_task_instance_id("collared_widget_view")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(
            title="Collared Subject Plot",
            **(params_dict.get("collared_widget_view") or {}),
        )
        .map(argnames=["view", "data"], argvalues=persist_collared_subject_plots)
    )

    grouped_collared_widget = (
        merge_widget_views.validate()
        .set_task_instance_id("grouped_collared_widget")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            widgets=collared_widget_view,
            **(params_dict.get("grouped_collared_widget") or {}),
        )
        .call()
    )

    generate_mcp_asymp_plot = (
        draw_season_mcp_plot.validate()
        .set_task_instance_id("generate_mcp_asymp_plot")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(**(params_dict.get("generate_mcp_asymp_plot") or {}))
        .mapvalues(
            argnames=["seasons_df", "relocations_gdf"],
            argvalues=zip_relocs_with_seasons,
        )
    )

    persist_mcp_html_urls = (
        persist_text.validate()
        .set_task_instance_id("persist_mcp_html_urls")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filename_suffix="mcp_asymptote_plot",
            **(params_dict.get("persist_mcp_html_urls") or {}),
        )
        .mapvalues(argnames=["text"], argvalues=generate_mcp_asymp_plot)
    )

    mcp_plot_widgets_single_view = (
        create_map_widget_single_view.validate()
        .set_task_instance_id("mcp_plot_widgets_single_view")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(
            title="MCP Asymptote",
            **(params_dict.get("mcp_plot_widgets_single_view") or {}),
        )
        .map(argnames=["view", "data"], argvalues=persist_mcp_html_urls)
    )

    grouped_mcp_plot_widget = (
        merge_widget_views.validate()
        .set_task_instance_id("grouped_mcp_plot_widget")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            widgets=mcp_plot_widgets_single_view,
            **(params_dict.get("grouped_mcp_plot_widget") or {}),
        )
        .call()
    )

    zip_traj_etd_gdf = (
        zip_groupbykey.validate()
        .set_task_instance_id("zip_traj_etd_gdf")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            sequences=[generate_etd, split_traj_by_group],
            **(params_dict.get("zip_traj_etd_gdf") or {}),
        )
        .call()
    )

    generate_subject_stats = (
        compute_subject_stats.validate()
        .set_task_instance_id("generate_subject_stats")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            output_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            subject_df=compute_subject_maturity,
            groupby_col="subject_name",
            **(params_dict.get("generate_subject_stats") or {}),
        )
        .mapvalues(argnames=["etd_df", "traj_gdf"], argvalues=zip_traj_etd_gdf)
    )

    persist_subject_stats = (
        persist_df.validate()
        .set_task_instance_id("persist_subject_stats")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filetype="csv",
            filename=None,
            **(params_dict.get("persist_subject_stats") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=generate_subject_stats)
    )

    zip_etd_subject_df = (
        zip_groupbykey.validate()
        .set_task_instance_id("zip_etd_subject_df")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            sequences=[generate_etd, split_subject_by_group],
            **(params_dict.get("zip_etd_subject_df") or {}),
        )
        .call()
    )

    build_regional_lookup = (
        build_template_region_lookup.validate()
        .set_task_instance_id("build_regional_lookup")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            gdf=load_ldx,
            categories=None,
            static_ids=None,
            **(params_dict.get("build_regional_lookup") or {}),
        )
        .call()
    )

    comp_template_regions = (
        compute_template_regions.validate()
        .set_task_instance_id("comp_template_regions")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            geodataframe=load_ldx,
            template_lookup=build_regional_lookup,
            crs="ESRI:53042",
            **(params_dict.get("comp_template_regions") or {}),
        )
        .call()
    )

    process_subject_occupancy = (
        compute_subject_occupancy.validate()
        .set_task_instance_id("process_subject_occupancy")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            crs="ESRI:53042",
            regions_gdf=comp_template_regions,
            output_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            **(params_dict.get("process_subject_occupancy") or {}),
        )
        .mapvalues(argnames=["etd_gdf", "subjects_df"], argvalues=zip_etd_subject_df)
    )

    persist_subject_occupancy = (
        persist_df.validate()
        .set_task_instance_id("persist_subject_occupancy")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filetype="csv",
            filename=None,
            **(params_dict.get("persist_subject_occupancy") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=process_subject_occupancy)
    )

    total_national_pa_use = (
        dataframe_column_sum.validate()
        .set_task_instance_id("total_national_pa_use")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            column_name="national_pa_use",
            **(params_dict.get("total_national_pa_use") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=process_subject_occupancy)
    )

    national_pa_quantity = (
        to_quantity.validate()
        .set_task_instance_id("national_pa_quantity")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(unit="%", **(params_dict.get("national_pa_quantity") or {}))
        .mapvalues(argnames=["value"], argvalues=total_national_pa_use)
    )

    total_pa_sv_widgets = (
        create_single_value_widget_single_view.validate()
        .set_task_instance_id("total_pa_sv_widgets")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(
            title="National protected area use",
            decimal_places=2,
            **(params_dict.get("total_pa_sv_widgets") or {}),
        )
        .map(argnames=["view", "data"], argvalues=national_pa_quantity)
    )

    national_pa_grouped_sv_widget = (
        merge_widget_views.validate()
        .set_task_instance_id("national_pa_grouped_sv_widget")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            widgets=total_pa_sv_widgets,
            **(params_dict.get("national_pa_grouped_sv_widget") or {}),
        )
        .call()
    )

    total_community_pa_use = (
        dataframe_column_sum.validate()
        .set_task_instance_id("total_community_pa_use")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            column_name="community_pa_use",
            **(params_dict.get("total_community_pa_use") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=process_subject_occupancy)
    )

    community_pa_quantity = (
        to_quantity.validate()
        .set_task_instance_id("community_pa_quantity")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(unit="%", **(params_dict.get("community_pa_quantity") or {}))
        .mapvalues(argnames=["value"], argvalues=total_community_pa_use)
    )

    total_community_pa_sv_widgets = (
        create_single_value_widget_single_view.validate()
        .set_task_instance_id("total_community_pa_sv_widgets")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(
            title="Community protected area use",
            decimal_places=2,
            **(params_dict.get("total_community_pa_sv_widgets") or {}),
        )
        .map(argnames=["view", "data"], argvalues=community_pa_quantity)
    )

    community_pa_grouped_sv_widget = (
        merge_widget_views.validate()
        .set_task_instance_id("community_pa_grouped_sv_widget")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            widgets=total_community_pa_sv_widgets,
            **(params_dict.get("community_pa_grouped_sv_widget") or {}),
        )
        .call()
    )

    total_crop_raid_percent = (
        dataframe_column_sum.validate()
        .set_task_instance_id("total_crop_raid_percent")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            column_name="crop_raid_percent",
            **(params_dict.get("total_crop_raid_percent") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=process_subject_occupancy)
    )

    crop_raid_percent_quantity = (
        to_quantity.validate()
        .set_task_instance_id("crop_raid_percent_quantity")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(unit="%", **(params_dict.get("crop_raid_percent_quantity") or {}))
        .mapvalues(argnames=["value"], argvalues=total_crop_raid_percent)
    )

    total_crop_raid_sv_widgets = (
        create_single_value_widget_single_view.validate()
        .set_task_instance_id("total_crop_raid_sv_widgets")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(
            title="Agricultural land use",
            decimal_places=2,
            **(params_dict.get("total_crop_raid_sv_widgets") or {}),
        )
        .map(argnames=["view", "data"], argvalues=crop_raid_percent_quantity)
    )

    crop_raid_sv_widget = (
        merge_widget_views.validate()
        .set_task_instance_id("crop_raid_sv_widget")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            widgets=total_crop_raid_sv_widgets,
            **(params_dict.get("crop_raid_sv_widget") or {}),
        )
        .call()
    )

    total_kenya_use = (
        dataframe_column_sum.validate()
        .set_task_instance_id("total_kenya_use")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(column_name="kenya_use", **(params_dict.get("total_kenya_use") or {}))
        .mapvalues(argnames=["df"], argvalues=process_subject_occupancy)
    )

    kenya_use_quantity = (
        to_quantity.validate()
        .set_task_instance_id("kenya_use_quantity")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(unit="%", **(params_dict.get("kenya_use_quantity") or {}))
        .mapvalues(argnames=["value"], argvalues=total_kenya_use)
    )

    total_kenya_use_sv_widgets = (
        create_single_value_widget_single_view.validate()
        .set_task_instance_id("total_kenya_use_sv_widgets")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(
            title="Kenya use",
            decimal_places=2,
            **(params_dict.get("total_kenya_use_sv_widgets") or {}),
        )
        .map(argnames=["view", "data"], argvalues=kenya_use_quantity)
    )

    kenya_use_grouped_sv_widget = (
        merge_widget_views.validate()
        .set_task_instance_id("kenya_use_grouped_sv_widget")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            widgets=total_kenya_use_sv_widgets,
            **(params_dict.get("kenya_use_grouped_sv_widget") or {}),
        )
        .call()
    )

    total_unprotected_use = (
        dataframe_column_sum.validate()
        .set_task_instance_id("total_unprotected_use")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            column_name="unprotected",
            **(params_dict.get("total_unprotected_use") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=process_subject_occupancy)
    )

    unprotected_quantity = (
        to_quantity.validate()
        .set_task_instance_id("unprotected_quantity")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(unit="%", **(params_dict.get("unprotected_quantity") or {}))
        .mapvalues(argnames=["value"], argvalues=total_unprotected_use)
    )

    total_unprotected_sv_widgets = (
        create_single_value_widget_single_view.validate()
        .set_task_instance_id("total_unprotected_sv_widgets")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(
            title="Unprotected use",
            decimal_places=2,
            **(params_dict.get("total_unprotected_sv_widgets") or {}),
        )
        .map(argnames=["view", "data"], argvalues=unprotected_quantity)
    )

    unprotected_grouped_sv_widget = (
        merge_widget_views.validate()
        .set_task_instance_id("unprotected_grouped_sv_widget")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            widgets=total_unprotected_sv_widgets,
            **(params_dict.get("unprotected_grouped_sv_widget") or {}),
        )
        .call()
    )

    compute_subject_mcp = (
        dataframe_column_sum.validate()
        .set_task_instance_id("compute_subject_mcp")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(column_name="MCP", **(params_dict.get("compute_subject_mcp") or {}))
        .mapvalues(argnames=["df"], argvalues=generate_subject_stats)
    )

    subject_mcp_quantity = (
        to_quantity.validate()
        .set_task_instance_id("subject_mcp_quantity")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(unit="kmÂ²", **(params_dict.get("subject_mcp_quantity") or {}))
        .mapvalues(argnames=["value"], argvalues=compute_subject_mcp)
    )

    total_mcp_sv_widgets = (
        create_single_value_widget_single_view.validate()
        .set_task_instance_id("total_mcp_sv_widgets")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(
            title="MCP Area",
            decimal_places=2,
            **(params_dict.get("total_mcp_sv_widgets") or {}),
        )
        .map(argnames=["view", "data"], argvalues=subject_mcp_quantity)
    )

    subject_mcp_grouped_sv_widget = (
        merge_widget_views.validate()
        .set_task_instance_id("subject_mcp_grouped_sv_widget")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            widgets=total_mcp_sv_widgets,
            **(params_dict.get("subject_mcp_grouped_sv_widget") or {}),
        )
        .call()
    )

    compute_subject_etd = (
        dataframe_column_sum.validate()
        .set_task_instance_id("compute_subject_etd")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(column_name="ETD", **(params_dict.get("compute_subject_etd") or {}))
        .mapvalues(argnames=["df"], argvalues=generate_subject_stats)
    )

    subject_etd_quantity = (
        to_quantity.validate()
        .set_task_instance_id("subject_etd_quantity")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(unit="kmÂ²", **(params_dict.get("subject_etd_quantity") or {}))
        .mapvalues(argnames=["value"], argvalues=compute_subject_etd)
    )

    total_etd_sv_widgets = (
        create_single_value_widget_single_view.validate()
        .set_task_instance_id("total_etd_sv_widgets")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(
            title="ETD",
            decimal_places=2,
            **(params_dict.get("total_etd_sv_widgets") or {}),
        )
        .map(argnames=["view", "data"], argvalues=subject_etd_quantity)
    )

    subject_etd_grouped_sv_widget = (
        merge_widget_views.validate()
        .set_task_instance_id("subject_etd_grouped_sv_widget")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            widgets=total_etd_sv_widgets,
            **(params_dict.get("subject_etd_grouped_sv_widget") or {}),
        )
        .call()
    )

    compute_sdt = (
        dataframe_column_sum.validate()
        .set_task_instance_id("compute_sdt")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            column_name="distance_travelled", **(params_dict.get("compute_sdt") or {})
        )
        .mapvalues(argnames=["df"], argvalues=generate_subject_stats)
    )

    subject_dtq = (
        to_quantity.validate()
        .set_task_instance_id("subject_dtq")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(unit="kmÂ²", **(params_dict.get("subject_dtq") or {}))
        .mapvalues(argnames=["value"], argvalues=compute_sdt)
    )

    tdt_sv_widgets = (
        create_single_value_widget_single_view.validate()
        .set_task_instance_id("tdt_sv_widgets")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(
            title="Distance Travelled",
            decimal_places=2,
            **(params_dict.get("tdt_sv_widgets") or {}),
        )
        .map(argnames=["view", "data"], argvalues=subject_dtq)
    )

    sdtg_sv_widget = (
        merge_widget_views.validate()
        .set_task_instance_id("sdtg_sv_widget")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(widgets=tdt_sv_widgets, **(params_dict.get("sdtg_sv_widget") or {}))
        .call()
    )

    compute_subject_max_displacement = (
        dataframe_column_sum.validate()
        .set_task_instance_id("compute_subject_max_displacement")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            column_name="max_displacement",
            **(params_dict.get("compute_subject_max_displacement") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=generate_subject_stats)
    )

    smd_quantity = (
        to_quantity.validate()
        .set_task_instance_id("smd_quantity")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(unit="kmÂ²", **(params_dict.get("smd_quantity") or {}))
        .mapvalues(argnames=["value"], argvalues=compute_subject_max_displacement)
    )

    tmd_sv_widgets = (
        create_single_value_widget_single_view.validate()
        .set_task_instance_id("tmd_sv_widgets")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(
            title="Max Displacement",
            decimal_places=2,
            **(params_dict.get("tmd_sv_widgets") or {}),
        )
        .map(argnames=["view", "data"], argvalues=smd_quantity)
    )

    smdg_sv_widget = (
        merge_widget_views.validate()
        .set_task_instance_id("smdg_sv_widget")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(widgets=tmd_sv_widgets, **(params_dict.get("smdg_sv_widget") or {}))
        .call()
    )

    compute_subject_night_day_ratio = (
        dataframe_column_sum.validate()
        .set_task_instance_id("compute_subject_night_day_ratio")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            column_name="night_day_ratio",
            **(params_dict.get("compute_subject_night_day_ratio") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=generate_subject_stats)
    )

    total_night_day_ratio_sv_widgets = (
        create_single_value_widget_single_view.validate()
        .set_task_instance_id("total_night_day_ratio_sv_widgets")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(
            title="Night Day Ratio",
            decimal_places=2,
            **(params_dict.get("total_night_day_ratio_sv_widgets") or {}),
        )
        .map(argnames=["view", "data"], argvalues=compute_subject_night_day_ratio)
    )

    sndrs_sv_widget = (
        merge_widget_views.validate()
        .set_task_instance_id("sndrs_sv_widget")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            widgets=total_night_day_ratio_sv_widgets,
            **(params_dict.get("sndrs_sv_widget") or {}),
        )
        .call()
    )

    zip_hr_value = (
        zip_groupbykey.validate()
        .set_task_instance_id("zip_hr_value")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            sequences=[zoom_speed_gdf_extent, persist_homerange_html],
            **(params_dict.get("zip_hr_value") or {}),
        )
        .call()
    )

    convert_homerange_png = (
        zoom_map_and_screenshot.validate()
        .set_task_instance_id("convert_homerange_png")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            screenshot_config={
                "full_page": False,
                "device_scale_factor": 2.0,
                "wait_for_timeout": 40000,
                "max_concurrent_pages": 1,
                "width": 602,
                "height": 855,
            },
            **(params_dict.get("convert_homerange_png") or {}),
        )
        .mapvalues(argnames=["view_state", "input_file"], argvalues=zip_hr_value)
    )

    zip_speed_value = (
        zip_groupbykey.validate()
        .set_task_instance_id("zip_speed_value")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            sequences=[zoom_speed_gdf_extent, persist_speedmap_html],
            **(params_dict.get("zip_speed_value") or {}),
        )
        .call()
    )

    convert_speedmap_png = (
        zoom_map_and_screenshot.validate()
        .set_task_instance_id("convert_speedmap_png")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            screenshot_config={
                "full_page": False,
                "device_scale_factor": 2.0,
                "wait_for_timeout": 40000,
                "max_concurrent_pages": 1,
                "width": 1280,
                "height": 720,
            },
            **(params_dict.get("convert_speedmap_png") or {}),
        )
        .mapvalues(argnames=["view_state", "input_file"], argvalues=zip_speed_value)
    )

    zip_seasonal_value = (
        zip_groupbykey.validate()
        .set_task_instance_id("zip_seasonal_value")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            sequences=[zoom_speed_gdf_extent, persist_seasonal_home_range_html],
            **(params_dict.get("zip_seasonal_value") or {}),
        )
        .call()
    )

    convert_season_png = (
        zoom_map_and_screenshot.validate()
        .set_task_instance_id("convert_season_png")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            screenshot_config={
                "full_page": False,
                "device_scale_factor": 2.0,
                "wait_for_timeout": 40000,
                "max_concurrent_pages": 1,
                "width": 602,
                "height": 855,
            },
            **(params_dict.get("convert_season_png") or {}),
        )
        .mapvalues(argnames=["view_state", "input_file"], argvalues=zip_seasonal_value)
    )

    convert_nsd_png = (
        html_to_png.validate()
        .set_task_instance_id("convert_nsd_png")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            config={
                "full_page": False,
                "device_scale_factor": 2.0,
                "wait_for_timeout": 5,
                "max_concurrent_pages": 3,
                "width": 2238,
                "height": 450,
            },
            **(params_dict.get("convert_nsd_png") or {}),
        )
        .mapvalues(argnames=["html_path"], argvalues=persist_nsd_html_urls)
    )

    convert_mcp_png = (
        html_to_png.validate()
        .set_task_instance_id("convert_mcp_png")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            config={
                "full_page": False,
                "device_scale_factor": 2.0,
                "wait_for_timeout": 5,
                "max_concurrent_pages": 1,
                "width": 2238,
                "height": 450,
            },
            **(params_dict.get("convert_mcp_png") or {}),
        )
        .mapvalues(argnames=["html_path"], argvalues=persist_mcp_html_urls)
    )

    convert_speed_png = (
        html_to_png.validate()
        .set_task_instance_id("convert_speed_png")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            config={
                "full_page": False,
                "device_scale_factor": 2.0,
                "wait_for_timeout": 5,
                "max_concurrent_pages": 1,
                "width": 2238,
                "height": 450,
            },
            **(params_dict.get("convert_speed_png") or {}),
        )
        .mapvalues(argnames=["html_path"], argvalues=persist_speed_html_urls)
    )

    convert_events_png = (
        html_to_png.validate()
        .set_task_instance_id("convert_events_png")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            config={
                "full_page": False,
                "device_scale_factor": 2.0,
                "wait_for_timeout": 5,
                "max_concurrent_pages": 1,
                "width": 2238,
                "height": 450,
            },
            **(params_dict.get("convert_events_png") or {}),
        )
        .mapvalues(argnames=["html_path"], argvalues=persist_collared_subject_plots)
    )

    unique_subjects = (
        dataframe_column_nunique.validate()
        .set_task_instance_id("unique_subjects")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=rename_traj_cols,
            column_name="subject_name",
            **(params_dict.get("unique_subjects") or {}),
        )
        .call()
    )

    download_cover_page = (
        fetch_and_persist_file.validate()
        .set_task_instance_id("download_cover_page")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            url="https://www.dropbox.com/scl/fi/nfv96xs38r3wunp6y866f/cer_cover_page.docx?rlkey=sbl545v87g94tolfafwyfd8b8&st=oyydvpy9&dl=0",
            output_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            overwrite_existing=False,
            unzip=False,
            retries=2,
            **(params_dict.get("download_cover_page") or {}),
        )
        .call()
    )

    download_sect_templates = (
        fetch_and_persist_file.validate()
        .set_task_instance_id("download_sect_templates")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            url="https://www.dropbox.com/scl/fi/4symf1385ksnh8mu8sx9v/mep_subject_template_two.docx?rlkey=v5f26c3aiadaasnilhc76owgr&st=wz6mce8l&dl=0",
            output_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            overwrite_existing=False,
            unzip=False,
            retries=2,
            **(params_dict.get("download_sect_templates") or {}),
        )
        .call()
    )

    create_cover_tpl_context = (
        create_mep_ctx_cover.validate()
        .set_task_instance_id("create_cover_tpl_context")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            count=unique_subjects,
            report_period=time_range,
            prepared_by="Ecoscope",
            **(params_dict.get("create_cover_tpl_context") or {}),
        )
        .call()
    )

    persist_cover_context = (
        create__mep_context_page.validate()
        .set_task_instance_id("persist_cover_context")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            template_path=download_cover_page,
            output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            context=create_cover_tpl_context,
            filename="mep_context.docx",
            **(params_dict.get("persist_cover_context") or {}),
        )
        .call()
    )

    group_subject_report_context = (
        zip_groupbykey.validate()
        .set_task_instance_id("group_subject_report_context")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            sequences=[
                download_profile_pic,
                persist_subject_info,
                convert_speedmap_png,
                convert_homerange_png,
                convert_season_png,
                convert_nsd_png,
                convert_speed_png,
                convert_events_png,
                convert_mcp_png,
                persist_subject_stats,
                persist_subject_occupancy,
            ],
            **(params_dict.get("group_subject_report_context") or {}),
        )
        .call()
    )

    create_subject_context = (
        create_mep_subject_context.validate()
        .set_task_instance_id("create_subject_context")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(**(params_dict.get("create_subject_context") or {}))
        .mapvalues(
            argnames=[
                "profile_photo_path",
                "subject_info_path",
                "speedmap_path",
                "homerange_map_path",
                "seasonal_homerange_map_path",
                "nsd_plot_path",
                "speed_plot_path",
                "collared_event_plot_path",
                "mcp_plot_path",
                "subject_stats_table_path",
                "subject_occupancy_table_path",
            ],
            argvalues=group_subject_report_context,
        )
    )

    persist_subject_report_context = (
        create_mep_grouper_page.validate()
        .set_task_instance_id("persist_subject_report_context")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            template_path=download_sect_templates,
            output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filename=None,
            validate_images=True,
            **(params_dict.get("persist_subject_report_context") or {}),
        )
        .mapvalues(argnames=["context"], argvalues=create_subject_context)
    )

    merge_mep_docx = (
        merge_mapbook_files.validate()
        .set_task_instance_id("merge_mep_docx")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            cover_page_path=persist_cover_context,
            output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            context_page_items=persist_subject_report_context,
            filename=None,
            **(params_dict.get("merge_mep_docx") or {}),
        )
        .call()
    )

    collared_report_template = (
        gather_dashboard.validate()
        .set_task_instance_id("collared_report_template")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            details=workflow_details,
            widgets=[
                national_pa_grouped_sv_widget,
                community_pa_grouped_sv_widget,
                crop_raid_sv_widget,
                kenya_use_grouped_sv_widget,
                unprotected_grouped_sv_widget,
                subject_mcp_grouped_sv_widget,
                subject_etd_grouped_sv_widget,
                sdtg_sv_widget,
                smdg_sv_widget,
                sndrs_sv_widget,
                merge_speedmap_widgets,
                merge_homerange_widgets,
                merge_seasonal_hr_widgets,
                grouped_nsd_plot_widget,
                grouped_speed_plot_widget,
                grouped_collared_widget,
                grouped_mcp_plot_widget,
            ],
            time_range=time_range,
            groupers=groupers,
            **(params_dict.get("collared_report_template") or {}),
        )
        .call()
    )

    return collared_report_template
